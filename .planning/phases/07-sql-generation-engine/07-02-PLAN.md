---
phase: 07-sql-generation-engine
plan: 02
type: tdd
wave: 2
depends_on: ["07-01"]
files_modified:
  - src/data_architect/generation/dml.py
  - tests/test_dml.py
autonomous: true

must_haves:
  truths:
    - "build_anchor_merge() produces a MERGE/UPSERT statement that loads data from staging into anchor tables without duplicates"
    - "build_attribute_merge() produces a MERGE statement that handles historized attributes with SCD2 bitemporal tracking (close old row, insert new row)"
    - "build_knot_merge() produces an INSERT-or-ignore pattern for static reference data"
    - "build_tie_merge() produces a MERGE for relationship data with optional bitemporal tracking"
    - "All DML is idempotent -- re-running produces no duplicates and no errors"
    - "PostgreSQL uses INSERT...ON CONFLICT, SQL Server and Snowflake use MERGE syntax, via SQLGlot dialect transpilation"
    - "DML output is deterministic -- same spec produces byte-identical SQL on every run"
  artifacts:
    - path: "src/data_architect/generation/dml.py"
      provides: "DML AST builder functions for MERGE/UPSERT loading patterns"
      exports: ["build_anchor_merge", "build_attribute_merge", "build_knot_merge", "build_tie_merge", "generate_all_dml"]
    - path: "tests/test_dml.py"
      provides: "TDD tests for all DML builders"
  key_links:
    - from: "src/data_architect/generation/dml.py"
      to: "src/data_architect/generation/naming.py"
      via: "Table name functions for source/target references"
      pattern: "from data_architect\\.generation\\.naming"
    - from: "src/data_architect/generation/dml.py"
      to: "src/data_architect/generation/columns.py"
      via: "Column builder functions for metadata population in WHEN MATCHED/NOT MATCHED clauses"
      pattern: "from data_architect\\.generation\\.columns"
    - from: "src/data_architect/generation/dml.py"
      to: "sqlglot.expressions"
      via: "AST construction for MERGE statements"
      pattern: "import sqlglot"
    - from: "src/data_architect/generation/dml.py"
      to: "src/data_architect/generation/ddl.py"
      via: "DML staging source references must use same naming as DDL staging tables"
      pattern: "staging_table_name"
---

<objective>
Build SQLGlot AST builder functions that produce idempotent DML (MERGE/UPSERT) loading patterns for all Anchor Model entity types, with dialect-aware syntax (PostgreSQL ON CONFLICT vs SQL Server/Snowflake MERGE).

Purpose: DML generation completes the SQL generation pipeline. With DDL (07-01) creating tables and DML loading data, Phase 7 delivers a functional generation engine. The MERGE patterns implement SCD2 (slowly changing dimensions type 2) for historized attributes and idempotent inserts for static entities.

Output: `src/data_architect/generation/dml.py` with DML builders and comprehensive TDD test suite.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-sql-generation-engine/07-RESEARCH.md
@.planning/phases/07-sql-generation-engine/07-01-SUMMARY.md
@src/data_architect/generation/ddl.py
@src/data_architect/generation/columns.py
@src/data_architect/generation/naming.py
@src/data_architect/models/anchor.py
@src/data_architect/models/tie.py
@src/data_architect/models/knot.py
@src/data_architect/models/spec.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: RED -- Write failing DML/MERGE tests</name>
  <files>
    tests/test_dml.py
    src/data_architect/generation/dml.py
  </files>
  <action>
Create `tests/test_dml.py` with failing tests for all DML builder functions. Create stub `src/data_architect/generation/dml.py` with NotImplementedError stubs.

**MERGE/UPSERT pattern overview:**

For each entity type, the DML loads data from a staging source into the target anchor model table:
- **Source:** staging table (e.g., `stg_customers`)
- **Target:** anchor model table (e.g., `CU_Customer`)
- **Key:** identity column for matching (ON clause)
- **Action:** INSERT if new, UPDATE if changed (for historized), or INSERT-ignore for static

**Tests to write:**

**Anchor MERGE (from generation.dml):**
- `test_build_anchor_merge_has_merge_keyword`: Verify the generated AST contains a MERGE expression (or INSERT...ON CONFLICT for postgres).
- `test_build_anchor_merge_matches_on_identity`: Verify the ON clause matches on the anchor identity column (`{mnemonic}_ID`).
- `test_build_anchor_merge_inserts_metadata`: Verify the INSERT clause includes metadata columns with placeholder expressions (CURRENT_TIMESTAMP for metadata_recorded_at, etc.).
- `test_build_anchor_merge_postgres_uses_on_conflict`: When dialect="postgres", verify output uses `INSERT ... ON CONFLICT ({mnemonic}_ID) DO NOTHING` pattern (anchor is identity-only, nothing to update).
- `test_build_anchor_merge_tsql_uses_merge`: When dialect="tsql", verify output uses `MERGE ... WHEN NOT MATCHED THEN INSERT` syntax.
- `test_build_anchor_merge_deterministic`: Two calls produce identical SQL (GEN-08).

**Attribute MERGE (from generation.dml):**
- `test_build_attribute_merge_historized_scd2`: For a historized attribute (has timeRange), verify the MERGE pattern:
  - WHEN MATCHED AND source value differs from target value: Update `recorded_at` to close the old row's validity
  - WHEN NOT MATCHED: Insert new row with `changed_at` = source timestamp, `recorded_at` = CURRENT_TIMESTAMP
- `test_build_attribute_merge_static_simple_upsert`: For a static attribute (no timeRange), verify simple INSERT...ON CONFLICT DO UPDATE pattern.
- `test_build_attribute_merge_has_anchor_fk`: Verify the INSERT references the anchor FK column.
- `test_build_attribute_merge_multi_dialect`: Generate for postgres, tsql, snowflake -- all parse back without error.

**Knot MERGE (from generation.dml):**
- `test_build_knot_merge_insert_ignore`: Knots are static reference data. Verify INSERT...ON CONFLICT DO NOTHING (postgres) or MERGE...WHEN NOT MATCHED (tsql/snowflake).
- `test_build_knot_merge_has_value_column`: Verify the knot value column is included in the INSERT.

**Tie MERGE (from generation.dml):**
- `test_build_tie_merge_has_role_columns`: Verify all role FK columns appear in the MERGE key and INSERT.
- `test_build_tie_merge_historized_has_bitemporal`: If tie has timeRange, verify SCD2-like pattern with bitemporal columns.
- `test_build_tie_merge_static_insert_ignore`: If no timeRange, verify INSERT-ignore pattern (relationship either exists or doesn't).

**Integration (from generation.dml):**
- `test_generate_all_dml_returns_dict`: Given a Spec, `generate_all_dml(spec, "postgres")` returns dict[filename, sql_string].
- `test_generate_all_dml_deterministic_order`: Keys and values identical across two calls.
- `test_generate_all_dml_filenames_match_ddl`: DML filenames correspond to DDL entity names (e.g., `CU_Customer_load.sql` for anchor CU).

Run tests -- all should FAIL (RED).

Commit: `test(07-02): add failing DML/MERGE generation tests` with `--no-verify`.

**Important design decisions for DML:**
- **Staging table naming:** DML builders MUST use `staging_table_name()` from `generation/naming.py` (the same function used by DDL in 07-01) to derive staging source table names. This ensures DDL-created staging tables and DML-referenced staging sources always match. The default convention is `stg_{anchor_table_name}` (e.g., `stg_CU_Customer`), but this is centralized in `naming.py`, not hardcoded in DML builders. When `anchor.staging_mappings` is populated, use the explicit table name from the mapping via `staging_table_name()`.
- Metadata population in INSERT: `metadata_recorded_at` = `CURRENT_TIMESTAMP`, `metadata_recorded_by` = `'architect'` (literal), `metadata_id` = UUID placeholder or NULL (deterministic -- use literal `'architect-generated'` for now, Phase 8 adds keyset).
- DML file naming: `{entity_table_name}_load.sql` to distinguish from DDL files.
  </action>
  <verify>
- `uv run pytest tests/test_dml.py` runs and all tests FAIL (RED)
- `uv run ruff check src/data_architect/generation/dml.py` passes
- `uv run mypy src/data_architect/generation/dml.py` passes
  </verify>
  <done>
- tests/test_dml.py has 15+ failing tests covering all DML builder functions
- Stub src/data_architect/generation/dml.py exists with typed signatures
- RED commit exists
  </done>
</task>

<task type="auto">
  <name>Task 2: GREEN -- Implement DML builders to pass all tests</name>
  <files>
    src/data_architect/generation/dml.py
    src/data_architect/generation/__init__.py
  </files>
  <action>
Implement all DML builder functions to pass every test in tests/test_dml.py.

**1. `generation/dml.py` -- DML AST builders:**

**build_anchor_merge(anchor: Anchor, dialect: str) -> sge.Expression:**
- Target: anchor table (from naming.anchor_table_name)
- Source: staging table (from naming.staging_table_name -- ensures consistency with DDL-generated staging tables)
- Key: `{mnemonic}_ID`
- Pattern: INSERT if not exists (anchor is identity-only, nothing to update)
- For postgres: `INSERT INTO target SELECT ... FROM source ON CONFLICT (key) DO NOTHING`
- For tsql/snowflake: `MERGE target USING source ON target.key = source.key WHEN NOT MATCHED THEN INSERT ...`
- Include metadata columns with: `CURRENT_TIMESTAMP`, `'architect'`, `'architect-generated'`

**build_attribute_merge(anchor: Anchor, attribute: Attribute, dialect: str) -> sge.Expression:**
- Target: attribute table (from naming.attribute_table_name)
- Source: staging table (from naming.staging_table_name -- attributes load from anchor's staging source)
- Key: composite of `{anchor_mnemonic}_ID` + value column (for static) or `{anchor_mnemonic}_ID` + `changed_at` (for historized)
- **If historized (timeRange present):** SCD2 pattern
  - WHEN MATCHED AND source.value != target.value: No update needed in insert-only SCD2 (Anchor Modeling convention: attributes are append-only, old rows stay with their recorded_at)
  - WHEN NOT MATCHED: INSERT with changed_at from source, recorded_at = CURRENT_TIMESTAMP
  - This follows Anchor Modeling's append-only attribute convention: we never update existing attribute rows. New values are simply inserted with a new changed_at timestamp. The latest value is determined by MAX(changed_at).
- **If static (no timeRange):** Simple UPSERT
  - ON CONFLICT DO UPDATE SET value = EXCLUDED.value, metadata columns updated

**build_knot_merge(knot: Knot, dialect: str) -> sge.Expression:**
- Target: knot table (from naming.knot_table_name)
- Source: staging table (from naming.staging_table_name)
- Key: `{mnemonic}_ID`
- Pattern: INSERT-ignore (knots are static reference data, never updated)

**build_tie_merge(tie: Tie, dialect: str) -> sge.Expression:**
- Target: tie table (from naming.tie_table_name)
- Source: constructed from role references
- Key: all role FK columns (composite key)
- **If historized:** Append-only like attributes (insert new version, don't update old)
- **If static:** INSERT-ignore (relationship exists or doesn't)

**generate_all_dml(spec: Spec, dialect: str) -> dict[str, str]:**
- Iterate in same deterministic order as DDL (GEN-08):
  1. Knots (sorted by mnemonic)
  2. Anchors (sorted by mnemonic) + their attributes (sorted by mnemonic)
  3. Ties (sorted by tie_table_name)
- File naming: `{table_name}_load.sql`
- Compile each AST with `.sql(dialect=dialect, pretty=True)`

**2. Update `generation/__init__.py`:**
Re-export `generate_all_dml` and individual DML builders.

**SQLGlot MERGE construction approach:**
Building MERGE ASTs directly with SQLGlot expressions can be complex. Use this approach:
1. Build the MERGE as a SQL string template with placeholders for table/column names
2. Parse with `sqlglot.parse_one(sql, dialect=dialect)`
3. Verify the AST round-trips correctly

Alternatively, construct the AST programmatically using `sge.Merge()` if SQLGlot supports it well. Test both approaches and use whichever produces cleaner, more maintainable code.

**For PostgreSQL (which uses INSERT...ON CONFLICT instead of MERGE for simple cases):**
Use `sge.Insert` with `sge.OnConflict` for simple patterns. For complex SCD2, PostgreSQL 15+ supports MERGE -- use that.

Run `uv run pytest tests/test_dml.py` until all pass, then `make check`.

Commit: `feat(07-02): implement SQLGlot DML/MERGE builders for anchor model loading`
  </action>
  <verify>
- `uv run pytest tests/test_dml.py` -- ALL tests pass (GREEN)
- `make check` passes (full quality gate)
- Manual: `uv run python -c "from data_architect.generation.dml import build_anchor_merge; from data_architect.models.anchor import Anchor; a = Anchor(mnemonic='CU', descriptor='Customer', identity='bigint'); print(build_anchor_merge(a, 'postgres').sql(dialect='postgres', pretty=True))"` produces valid MERGE/UPSERT SQL
  </verify>
  <done>
- All DML builder functions implemented and passing tests
- Anchor, attribute, knot, and tie MERGE/UPSERT patterns working
- SCD2 pattern for historized attributes (append-only per Anchor convention)
- Multi-dialect support (postgres, tsql, snowflake)
- Deterministic output (same spec = same SQL)
- make check fully green
- GREEN commit exists
  </done>
</task>

</tasks>

<verification>
1. `make check` passes with no warnings
2. `uv run pytest tests/test_dml.py -v` shows all 15+ tests passing
3. DML for postgres uses INSERT...ON CONFLICT syntax
4. DML for tsql uses MERGE syntax
5. Historized attributes use append-only SCD2 pattern
6. Static entities use INSERT-ignore pattern
7. Determinism: two runs produce byte-identical output
8. All DML includes metadata column population
</verification>

<success_criteria>
- generation/dml.py exists with MERGE/UPSERT builders for all entity types
- Pure functions: Pydantic model + dialect string in, SQLGlot AST out
- Idempotent DML: re-running produces no duplicates (GEN-05)
- Dialect-aware: PostgreSQL ON CONFLICT, SQL Server/Snowflake MERGE
- TDD: RED commit with failing tests, GREEN commit with passing implementation
- Full quality gates pass (make check)
</success_criteria>

<output>
After completion, create `.planning/phases/07-sql-generation-engine/07-02-SUMMARY.md`
</output>
