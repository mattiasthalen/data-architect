---
phase: 07-sql-generation-engine
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - uv.lock
  - src/data_architect/generation/__init__.py
  - src/data_architect/generation/columns.py
  - src/data_architect/generation/naming.py
  - src/data_architect/generation/ddl.py
  - tests/test_ddl.py
  - tests/fixtures/valid_spec.yaml
autonomous: true

must_haves:
  truths:
    - "build_anchor_table() produces CREATE TABLE IF NOT EXISTS with identity column, bitemporal columns, and metadata columns for any dialect"
    - "build_attribute_table() produces a separate table per historized attribute with FK to anchor, value column, bitemporal and metadata columns"
    - "build_knot_table() produces a lookup table with identity, value, and metadata columns"
    - "build_tie_table() produces a relationship table with FK columns per role, optional bitemporal columns, and metadata columns"
    - "build_staging_table() produces CREATE TABLE IF NOT EXISTS from staging mapping definitions in the spec"
    - "All DDL generation is deterministic -- sorted by mnemonic, no timestamps/UUIDs, byte-identical on repeated runs"
    - "Generated SQL compiles to PostgreSQL, SQL Server (tsql), and Snowflake dialects via SQLGlot transpilation"
  artifacts:
    - path: "src/data_architect/generation/ddl.py"
      provides: "DDL AST builder functions for all Anchor Model entity types"
      exports: ["build_anchor_table", "build_attribute_table", "build_knot_table", "build_tie_table", "build_staging_table", "generate_all_ddl"]
    - path: "src/data_architect/generation/columns.py"
      provides: "Reusable column builder functions for bitemporal and metadata columns"
      exports: ["build_bitemporal_columns", "build_metadata_columns"]
    - path: "src/data_architect/generation/naming.py"
      provides: "Deterministic naming conventions for tables and files"
      exports: ["anchor_table_name", "attribute_table_name", "knot_table_name", "tie_table_name", "staging_table_name"]
    - path: "tests/test_ddl.py"
      provides: "TDD tests for all DDL builders"
  key_links:
    - from: "src/data_architect/generation/ddl.py"
      to: "src/data_architect/models/anchor.py"
      via: "Pydantic Anchor/Attribute models as input"
      pattern: "from data_architect\\.models"
    - from: "src/data_architect/generation/ddl.py"
      to: "src/data_architect/generation/columns.py"
      via: "Column builder imports"
      pattern: "from data_architect\\.generation\\.columns"
    - from: "src/data_architect/generation/ddl.py"
      to: "sqlglot.expressions"
      via: "AST construction"
      pattern: "import sqlglot"
---

<objective>
Build SQLGlot AST builder functions that transform validated Pydantic spec models into idempotent DDL (CREATE TABLE IF NOT EXISTS) for all Anchor Model entity types: anchors, attributes, ties, knots, and staging tables.

Purpose: This is the core SQL generation engine. Every downstream feature (DML loading, CLI output, Bruin formatting) depends on correct DDL generation. By building pure functions that map Pydantic models to SQLGlot ASTs, we get deterministic output, dialect-agnostic generation, and testable transformations.

Output: `src/data_architect/generation/` module with DDL builders, column builders, naming conventions, and comprehensive TDD test suite.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-sql-generation-engine/07-RESEARCH.md
@.planning/phases/06-yaml-schema-foundation-and-spec-validation/06-01-SUMMARY.md
@src/data_architect/models/anchor.py
@src/data_architect/models/tie.py
@src/data_architect/models/knot.py
@src/data_architect/models/spec.py
@src/data_architect/models/common.py
@tests/fixtures/valid_spec.yaml
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: RED -- Add sqlglot dependency and write failing DDL tests</name>
  <files>
    pyproject.toml
    tests/test_ddl.py
  </files>
  <action>
1. Add `sqlglot>=28.10.0` to `[project] dependencies` in pyproject.toml. Run `uv sync` to install.

2. Create `tests/test_ddl.py` with failing tests for all DDL builder functions. Import the functions that don't exist yet (will fail on import). Tests to write:

**Column builders (from generation.columns):**
- `test_bitemporal_columns_has_changed_at_and_recorded_at`: Verify `build_bitemporal_columns("postgres")` returns exactly 2 ColumnDef nodes named `changed_at` and `recorded_at`, both NOT NULL TIMESTAMPTZ.
- `test_metadata_columns_has_three_columns`: Verify `build_metadata_columns("postgres")` returns 3 ColumnDef nodes: `metadata_recorded_at` (TIMESTAMPTZ NOT NULL), `metadata_recorded_by` (VARCHAR(255)), `metadata_id` (VARCHAR(255)).
- `test_bitemporal_columns_snowflake_types`: Verify dialect="snowflake" produces TIMESTAMP_NTZ (or equivalent Snowflake type) instead of TIMESTAMPTZ.

**Naming conventions (from generation.naming):**
- `test_anchor_table_name`: `anchor_table_name(Anchor(mnemonic="CU", descriptor="Customer", identity="bigint"))` returns `"CU_Customer"`.
- `test_attribute_table_name`: `attribute_table_name(Anchor(mnemonic="CU", ...), Attribute(mnemonic="NAM", descriptor="Name", dataRange="varchar(100)"))` returns `"CU_NAM_Customer_Name"`.
- `test_knot_table_name`: `knot_table_name(Knot(mnemonic="GEN", descriptor="Gender", identity="int", dataRange="varchar(42)"))` returns `"GEN_Gender"`.
- `test_tie_table_name`: For a tie with roles referencing AC and PN, returns deterministic name based on sorted role types (e.g., `"AC_PN_subset_of"` -- sorted by type then role name).
- `test_staging_table_name`: Returns the staging table name as defined in the spec mapping.

**Anchor DDL (from generation.ddl):**
- `test_build_anchor_table_has_identity_pk`: Verify the CREATE TABLE contains a primary key column named `{mnemonic}_ID` with the specified identity type.
- `test_build_anchor_table_has_metadata_columns`: Verify metadata columns are present.
- `test_build_anchor_table_if_not_exists`: Verify the AST has `exists=True` (idempotent DDL per GEN-04).
- `test_build_anchor_table_deterministic`: Call twice on same input, assert `sql1 == sql2` (GEN-08).
- `test_build_anchor_table_multi_dialect`: Generate for postgres, tsql, snowflake -- all produce valid SQL (parse back without error).

**Attribute DDL (from generation.ddl):**
- `test_build_attribute_table_has_anchor_fk`: Verify FK column `{anchor_mnemonic}_ID` exists.
- `test_build_attribute_table_has_value_column`: Verify value column exists with correct data type (from dataRange) or FK reference (from knotRange).
- `test_build_attribute_table_historized_has_bitemporal`: If attribute has timeRange, verify bitemporal columns present.
- `test_build_attribute_table_static_no_bitemporal`: If attribute has no timeRange, verify no bitemporal columns (but metadata columns still present).

**Knot DDL (from generation.ddl):**
- `test_build_knot_table_has_identity_and_value`: Verify `{mnemonic}_ID` PK and `{mnemonic}_{descriptor}` value column.
- `test_build_knot_table_has_metadata`: Verify metadata columns present.

**Tie DDL (from generation.ddl):**
- `test_build_tie_table_has_role_columns`: For each role in tie, verify FK column `{role_type}_ID_{role_name}` exists.
- `test_build_tie_table_historized_has_bitemporal`: If tie has timeRange, verify bitemporal columns.
- `test_build_tie_table_static_no_bitemporal`: If no timeRange, no bitemporal columns.

**Staging DDL (from generation.ddl):**
- `test_build_staging_table_placeholder`: Since Phase 8 populates staging_mappings, test that `build_staging_table()` generates DDL from a minimal staging definition (table name + column list). For now, if no staging_mappings on any anchor, `generate_all_ddl` simply skips staging tables.

**Integration (from generation.ddl):**
- `test_generate_all_ddl_returns_dict_of_filename_to_sql`: Given a full Spec (use valid_spec.yaml fixture), verify `generate_all_ddl(spec, "postgres")` returns a dict mapping filenames to SQL strings, with one entry per entity.
- `test_generate_all_ddl_deterministic_order`: Call twice, assert dict keys are in same order and values are identical (GEN-08).

3. Create stub files so the import at least resolves:
   - `src/data_architect/generation/__init__.py` (empty)
   - `src/data_architect/generation/columns.py` with stub functions that raise NotImplementedError
   - `src/data_architect/generation/naming.py` with stub functions that raise NotImplementedError
   - `src/data_architect/generation/ddl.py` with stub functions that raise NotImplementedError

4. Run `uv run pytest tests/test_ddl.py` -- all tests should FAIL (RED state).

5. Commit: `test(07-01): add failing DDL generation tests` with `--no-verify` (tests intentionally fail).

**Anchor Model Column Ordering Convention** (apply in all table builders):
1. Identity column (PK)
2. FK/reference columns (anchor ID for attributes, role IDs for ties)
3. Value column (for attributes and knots)
4. Bitemporal columns (changed_at, recorded_at) -- only for historized entities
5. Metadata columns (metadata_recorded_at, metadata_recorded_by, metadata_id)

**Important implementation notes:**
- Use `sqlglot.expressions as sge` and `sqlglot as sg` per research patterns.
- Use `sge.DataType.build(type_str, dialect=dialect)` for dialect-aware type mapping.
- Use `sg.to_identifier(name)` for column/table names.
- All builder functions are pure: take Pydantic model + dialect string, return `sge.Create` AST node.
- The `generate_all_ddl(spec, dialect)` function sorts entities by mnemonic before iterating (GEN-08 determinism).
  </action>
  <verify>
- `uv sync` succeeds (sqlglot installed)
- `uv run pytest tests/test_ddl.py` runs and all tests FAIL (not error on import, but fail on NotImplementedError or assertion)
- `uv run ruff check src/data_architect/generation/` passes (stubs are clean)
- `uv run mypy src/data_architect/generation/` passes (stubs have type annotations)
  </verify>
  <done>
- sqlglot>=28.10.0 in pyproject.toml dependencies
- tests/test_ddl.py has 20+ failing tests covering all DDL builder functions
- Stub modules exist at generation/__init__.py, generation/columns.py, generation/naming.py, generation/ddl.py
- All stubs have proper type annotations and raise NotImplementedError
- RED commit exists
  </done>
</task>

<task type="auto">
  <name>Task 2: GREEN -- Implement DDL builders to pass all tests</name>
  <files>
    src/data_architect/generation/__init__.py
    src/data_architect/generation/columns.py
    src/data_architect/generation/naming.py
    src/data_architect/generation/ddl.py
    pyproject.toml
  </files>
  <action>
Implement all DDL builder functions to make every test in tests/test_ddl.py pass.

**1. `generation/columns.py` -- Reusable column builders:**

```python
def build_bitemporal_columns(dialect: str) -> list[sge.ColumnDef]:
    """Valid time (changed_at) + transaction time (recorded_at). GEN-06."""
    # Both TIMESTAMPTZ NOT NULL
    # Use sge.DataType.build("TIMESTAMPTZ", dialect=dialect) for dialect mapping

def build_metadata_columns(dialect: str) -> list[sge.ColumnDef]:
    """Metadata columns per GEN-07: metadata_recorded_at, metadata_recorded_by, metadata_id."""
    # metadata_recorded_at: TIMESTAMPTZ NOT NULL
    # metadata_recorded_by: VARCHAR(255)
    # metadata_id: VARCHAR(255)
```

**2. `generation/naming.py` -- Deterministic naming:**

Follow Anchor Modeling naming conventions:
- Anchor table: `{mnemonic}_{descriptor}` (e.g., `CU_Customer`)
- Attribute table: `{anchor_mnemonic}_{attr_mnemonic}_{anchor_descriptor}_{attr_descriptor}` (e.g., `CU_NAM_Customer_Name`)
- Knot table: `{mnemonic}_{descriptor}` (e.g., `GEN_Gender`)
- Tie table: Compose from sorted roles -- sort by `(role.type_, role.role)` then join with `_` (e.g., `AC_PN_subset_of`). This ensures deterministic naming regardless of YAML role order.
- File name: `{table_name}.sql` (lowercase)

**3. `generation/ddl.py` -- DDL AST builders:**

For each entity type, build a `sge.Create` AST with `exists=True` (IF NOT EXISTS):

**build_anchor_table(anchor: Anchor, dialect: str) -> sge.Create:**
- Identity PK column: `{mnemonic}_ID` with type from anchor.identity
- Metadata columns (always)
- No bitemporal columns on anchor itself (anchor is just the entity reference)

**build_attribute_table(anchor: Anchor, attribute: Attribute, dialect: str) -> sge.Create:**
- Anchor FK column: `{anchor_mnemonic}_ID` with type from anchor.identity (NOT NULL)
- Value column: If dataRange, column is `{anchor_mnemonic}_{attr_mnemonic}_{anchor_descriptor}_{attr_descriptor}` with type from dataRange. If knotRange, column is `{knot_mnemonic}_ID` with FK reference type.
- Bitemporal columns: ONLY if attribute.time_range is not None (historized attribute)
- Metadata columns (always)

**build_knot_table(knot: Knot, dialect: str) -> sge.Create:**
- Identity PK column: `{mnemonic}_ID` with type from knot.identity
- Value column: `{mnemonic}_{descriptor}` with type from knot.data_range
- Metadata columns (always)
- No bitemporal columns (knots are static reference data)

**build_tie_table(tie: Tie, dialect: str) -> sge.Create:**
- For each role: FK column `{role.type_}_ID_{role.role}` with type bigint (default FK type, actual type resolved by referencing anchor identity but we use bigint as convention for now)
- Bitemporal columns: ONLY if tie.time_range is not None
- Metadata columns (always)

**build_staging_table(name: str, columns: list[tuple[str, str]], dialect: str) -> sge.Create:**
- Simple CREATE TABLE IF NOT EXISTS with provided column name/type pairs
- Metadata columns (always)
- This is a minimal implementation -- Phase 8 will expand with staging_mappings

**generate_all_ddl(spec: Spec, dialect: str) -> dict[str, str]:**
- Iterate in deterministic order (GEN-08):
  1. Knots (sorted by mnemonic)
  2. Anchors (sorted by mnemonic) -- for each anchor, generate anchor table + attribute tables (sorted by mnemonic)
  3. Ties (sorted by tie_table_name)
  4. Staging tables (sorted by name, skip if no staging_mappings)
- For each entity, call the appropriate builder, compile to SQL string with `.sql(dialect=dialect, pretty=True)`
- Return dict[filename, sql_string]

**4. `generation/__init__.py` -- Public API:**
Re-export the main entry point: `generate_all_ddl` and individual builders for direct use.

**5. Add per-file ruff ignores if needed:**
- `generation/ddl.py`: May need TC001 if Pydantic models need runtime access
- `generation/columns.py`: Should be clean

Run `uv run pytest tests/test_ddl.py` until all tests pass. Then run full `make check` to ensure no regressions.

Commit: `feat(07-01): implement SQLGlot DDL builders for anchor model entities`
  </action>
  <verify>
- `uv run pytest tests/test_ddl.py` -- ALL tests pass (GREEN state)
- `make check` passes (lint + type + test for entire project)
- `uv run pytest --cov=data_architect --cov-report=term-missing` shows generation module covered
- Manual spot check: `uv run python -c "from data_architect.generation.ddl import build_anchor_table; from data_architect.models.anchor import Anchor; a = Anchor(mnemonic='CU', descriptor='Customer', identity='bigint'); print(build_anchor_table(a, 'postgres').sql(dialect='postgres', pretty=True))"` produces valid CREATE TABLE IF NOT EXISTS
  </verify>
  <done>
- All DDL builder functions implemented and passing tests
- Column ordering follows Anchor convention (identity, FKs, value, bitemporal, metadata)
- All generated DDL is idempotent (IF NOT EXISTS)
- Deterministic output verified (same input = same SQL)
- Multi-dialect compilation verified (postgres, tsql, snowflake)
- make check fully green
- GREEN commit exists
  </done>
</task>

</tasks>

<verification>
1. `make check` passes with no warnings
2. `uv run pytest tests/test_ddl.py -v` shows all 20+ tests passing
3. Manual generation test produces valid SQL for postgres, tsql, snowflake dialects
4. Determinism test: two runs of generate_all_ddl on same spec produce identical output
5. Every generated table includes metadata columns (GEN-07)
6. Historized entities include bitemporal columns, static entities do not (GEN-06)
7. All DDL uses IF NOT EXISTS (GEN-04)
</verification>

<success_criteria>
- SQLGlot dependency added and functional
- generation/ module exists with ddl.py, columns.py, naming.py
- Pure functions: Pydantic model + dialect string in, SQLGlot AST out
- Anchor, attribute, knot, tie, and staging table DDL all generate correctly
- TDD: RED commit with failing tests, GREEN commit with passing implementation
- Full quality gates pass (make check)
</success_criteria>

<output>
After completion, create `.planning/phases/07-sql-generation-engine/07-01-SUMMARY.md`
</output>
