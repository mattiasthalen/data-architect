---
phase: 07-sql-generation-engine
plan: 03
type: execute
wave: 3
depends_on: ["07-01", "07-02"]
files_modified:
  - src/data_architect/cli.py
  - src/data_architect/generation/__init__.py
  - src/data_architect/generation/formatters.py
  - tests/test_cli.py
  - tests/test_formatters.py
autonomous: true

must_haves:
  truths:
    - "Running `architect dab generate spec.yaml` produces SQL files in an output directory"
    - "`--format raw` produces plain .sql files, `--format bruin` produces SQL with Bruin YAML frontmatter"
    - "`--dialect postgres|tsql|snowflake` controls the SQL dialect of generated output"
    - "Default output directory is `./output/` relative to spec file, configurable with `--output-dir`"
    - "Generated files are written deterministically -- running twice produces identical output"
    - "Invalid spec file produces a clear error message with validation errors, not a stack trace"
  artifacts:
    - path: "src/data_architect/generation/formatters.py"
      provides: "Output formatting functions for raw and Bruin formats"
      exports: ["format_raw", "format_bruin", "write_output"]
    - path: "src/data_architect/cli.py"
      provides: "dab generate CLI command"
    - path: "tests/test_cli.py"
      provides: "CLI integration tests for dab generate"
    - path: "tests/test_formatters.py"
      provides: "Unit tests for output formatters"
  key_links:
    - from: "src/data_architect/cli.py"
      to: "src/data_architect/generation/__init__.py"
      via: "CLI calls generate_all_ddl and generate_all_dml"
      pattern: "from data_architect\\.generation"
    - from: "src/data_architect/cli.py"
      to: "src/data_architect/validation/loader.py"
      via: "CLI validates spec before generation"
      pattern: "from data_architect\\.validation"
    - from: "src/data_architect/generation/formatters.py"
      to: "src/data_architect/generation/ddl.py"
      via: "Formatters wrap DDL/DML SQL strings with Bruin frontmatter"
      pattern: "format_bruin"
    - from: "tests/test_cli.py"
      to: "src/data_architect/generation/naming.py"
      via: "Integration test validates DML staging source names match DDL staging table names"
      pattern: "staging.*match|staging.*ddl"
---

<objective>
Wire DDL and DML generation into the `architect dab generate` CLI command with output format flags (raw/bruin), dialect selection, and file writing. This completes the Phase 7 SQL generation pipeline end-to-end.

Purpose: With DDL and DML builders in place (07-01, 07-02), this plan connects them to the user-facing CLI, adds Bruin format support for pipeline orchestration, and handles file output with deterministic naming. After this plan, users can run `architect dab generate spec.yaml` and get working SQL.

Output: Extended CLI with `dab generate` command, formatters module, and integration tests.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-sql-generation-engine/07-RESEARCH.md
@.planning/phases/07-sql-generation-engine/07-01-SUMMARY.md
@.planning/phases/07-sql-generation-engine/07-02-SUMMARY.md
@src/data_architect/cli.py
@src/data_architect/generation/__init__.py
@src/data_architect/generation/ddl.py
@src/data_architect/generation/dml.py
@src/data_architect/generation/formatters.py
@src/data_architect/validation/loader.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement output formatters (raw + Bruin)</name>
  <files>
    src/data_architect/generation/formatters.py
    tests/test_formatters.py
  </files>
  <action>
Create `src/data_architect/generation/formatters.py` with output formatting functions and `tests/test_formatters.py` with tests.

**1. `format_raw(sql: str) -> str`:**
- Returns the SQL string as-is (identity function, but explicit for the format pipeline).
- Ensures consistent trailing newline.

**2. `format_bruin(sql: str, entity_name: str, entity_type: str, is_historized: bool) -> str`:**
- Wraps SQL with Bruin YAML frontmatter (per research patterns).
- Frontmatter structure:
  ```
  /* @bruin
  name: dab.{entity_name}
  type: sql
  materialization:
      type: table
      strategy: {strategy}
  @bruin */

  {sql}
  ```
- Strategy determination:
  - DDL files: `strategy: create+replace` (always recreate structure)
  - DML files with historized entities: `strategy: merge` (SCD2 append)
  - DML files with static entities: `strategy: create+replace`
- The `entity_type` param indicates: "ddl" or "dml"
- The `is_historized` param determines merge vs create+replace for DML

**3. `write_output(files: dict[str, str], output_dir: Path, format_fn: Callable) -> list[Path]`:**
- Takes dict[filename, sql_string] from generate_all_ddl/generate_all_dml
- Creates output_dir if it doesn't exist
- Writes each file using format_fn
- Returns list of written file paths (sorted, for deterministic output)
- Creates subdirectories: `output_dir/ddl/` and `output_dir/dml/`

**Tests for test_formatters.py:**
- `test_format_raw_returns_sql_with_trailing_newline`: Plain SQL returned with `\n` at end.
- `test_format_bruin_has_frontmatter`: Verify `/* @bruin` and `@bruin */` delimiters present.
- `test_format_bruin_entity_name`: Verify `name: dab.{entity_name}` in frontmatter.
- `test_format_bruin_historized_uses_merge`: For DML + historized, verify `strategy: merge`.
- `test_format_bruin_static_uses_create_replace`: For DML + static, verify `strategy: create+replace`.
- `test_format_bruin_ddl_uses_create_replace`: DDL files always use `strategy: create+replace`.
- `test_write_output_creates_files`: Given a dict, verify files are written to disk.
- `test_write_output_creates_subdirs`: Verify ddl/ and dml/ subdirectories created.
- `test_write_output_deterministic`: Two writes produce identical files.

Run tests, then `make check`.

Commit: `feat(07-03): add raw and Bruin output formatters with file writing`
  </action>
  <verify>
- `uv run pytest tests/test_formatters.py -v` -- all tests pass
- `uv run ruff check src/data_architect/generation/formatters.py` passes
- `uv run mypy src/data_architect/generation/formatters.py` passes
  </verify>
  <done>
- formatters.py exists with format_raw, format_bruin, write_output
- Bruin frontmatter correctly uses merge/create+replace strategies
- File writing is deterministic and creates proper directory structure
- All formatter tests pass
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire `dab generate` CLI command with format and dialect flags</name>
  <files>
    src/data_architect/cli.py
    src/data_architect/generation/__init__.py
    tests/test_cli.py
    pyproject.toml
  </files>
  <action>
Add the `dab generate` command to the existing CLI, wiring validation, generation, and formatting together.

**1. Update `src/data_architect/cli.py`:**

Add `dab_generate` command to the existing `dab_app` Typer sub-app:

```python
from enum import StrEnum

class OutputFormat(StrEnum):
    RAW = "raw"
    BRUIN = "bruin"

class Dialect(StrEnum):
    POSTGRES = "postgres"
    TSQL = "tsql"
    SNOWFLAKE = "snowflake"

@dab_app.command(name="generate")
def dab_generate(
    spec_path: Path = typer.Argument(..., help="Path to YAML spec file"),
    output_dir: Path = typer.Option(
        None,
        "--output-dir", "-o",
        help="Output directory (default: ./output/ relative to spec)",
    ),
    format: OutputFormat = typer.Option(
        OutputFormat.RAW,
        "--format", "-f",
        help="Output format: raw (plain SQL) or bruin (SQL with YAML frontmatter)",
    ),
    dialect: Dialect = typer.Option(
        Dialect.POSTGRES,
        "--dialect", "-d",
        help="SQL dialect: postgres, tsql, snowflake",
    ),
) -> None:
    """Generate SQL from a validated YAML spec."""
```

**Command flow:**
1. Validate spec file exists, error if not
2. Load and validate spec using `validate_spec()` from validation module
3. If validation errors, print formatted errors and exit(1)
4. Call `generate_all_ddl(spec, dialect)` to get DDL dict
5. Call `generate_all_dml(spec, dialect)` to get DML dict
6. Determine output_dir (default: spec_path.parent / "output")
7. Write DDL files to `output_dir/ddl/` using appropriate formatter
8. Write DML files to `output_dir/dml/` using appropriate formatter
9. Print summary: N DDL files, M DML files written to output_dir

**2. Update `generation/__init__.py`:**
Ensure all public functions are exported for CLI use:
- `generate_all_ddl`, `generate_all_dml`
- `format_raw`, `format_bruin`, `write_output`

**3. Add CLI integration tests to `tests/test_cli.py`:**

- `test_dab_generate_with_valid_spec`: Create tmp spec file (copy valid_spec.yaml fixture), run `architect dab generate spec.yaml`, verify output directory created with .sql files.
- `test_dab_generate_with_invalid_spec`: Run on a malformed spec, verify exit code 1 and error message contains validation errors.
- `test_dab_generate_missing_spec`: Run with nonexistent file, verify exit code 1 and "not found" error.
- `test_dab_generate_format_raw`: Verify `--format raw` produces plain SQL (no frontmatter).
- `test_dab_generate_format_bruin`: Verify `--format bruin` produces SQL with `/* @bruin` frontmatter.
- `test_dab_generate_dialect_postgres`: Default dialect produces PostgreSQL SQL.
- `test_dab_generate_dialect_snowflake`: `--dialect snowflake` produces Snowflake SQL.
- `test_dab_generate_custom_output_dir`: Verify `--output-dir /tmp/custom` writes to that directory.
- `test_dab_generate_deterministic`: Run twice on same spec, verify output files are byte-identical (GEN-08).
- `test_dab_generate_dml_staging_refs_match_ddl`: **Cross-validation test.** Given a spec with staging_mappings on an anchor, run `generate_all_ddl` and `generate_all_dml` on the same spec. Verify that every staging table name referenced in DML output (as a source table) has a corresponding staging table DDL file in the DDL output. This validates the naming convention linkage between DDL (07-01) and DML (07-02).
- `test_dab_generate_help`: Verify `architect dab generate --help` shows format and dialect options.

**4. Update pyproject.toml if needed:**
- Add per-file ignores for generation module files if ruff flags arise
- Ensure cli.py B008 ignore covers new Typer option defaults

All tests must pass. Run `make check` for full quality gate.

Commit: `feat(07-03): wire dab generate CLI with format and dialect flags`
  </action>
  <verify>
- `uv run pytest tests/test_cli.py -v -k "dab_generate"` -- all new CLI tests pass
- `make check` passes (full quality gate)
- Manual end-to-end: `uv run architect dab generate tests/fixtures/valid_spec.yaml --format raw --dialect postgres` produces .sql files in output/
- Manual Bruin test: `uv run architect dab generate tests/fixtures/valid_spec.yaml --format bruin` produces SQL with frontmatter
- `architect dab generate --help` shows all options
  </verify>
  <done>
- `architect dab generate` command exists and works end-to-end
- `--format raw|bruin` flag controls output format
- `--dialect postgres|tsql|snowflake` flag controls SQL dialect
- `--output-dir` flag controls output location
- Invalid spec produces clear validation errors
- Deterministic output verified
- All CLI integration tests pass
- make check fully green
  </done>
</task>

</tasks>

<verification>
1. `make check` passes with no warnings
2. Full end-to-end: `architect dab generate spec.yaml` produces DDL + DML SQL files
3. `--format bruin` adds correct Bruin frontmatter
4. `--format raw` produces clean SQL without wrapper
5. `--dialect postgres` uses PostgreSQL syntax, `--dialect snowflake` uses Snowflake syntax
6. Invalid spec shows validation errors with line numbers
7. Missing spec file shows clear error
8. Deterministic: two runs on same spec produce identical output directory
9. Output organized in ddl/ and dml/ subdirectories
</verification>

<success_criteria>
- `architect dab generate` is a working CLI command
- Users can generate SQL from YAML spec with one command
- Format flag (raw/bruin) and dialect flag (postgres/tsql/snowflake) work correctly
- Bruin frontmatter uses correct materialization strategies
- Output directory structure is clean and predictable
- Error handling is user-friendly (validation errors, missing files)
- Full quality gates pass (make check)
- All GEN-09 requirements met
</success_criteria>

<output>
After completion, create `.planning/phases/07-sql-generation-engine/07-03-SUMMARY.md`
</output>
