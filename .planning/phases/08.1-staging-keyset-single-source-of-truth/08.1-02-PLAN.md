---
phase: 08.1-staging-keyset-single-source-of-truth
plan: 02
type: execute
wave: 2
depends_on: ["08.1-01"]
files_modified:
  - src/data_architect/generation/dml.py
  - tests/test_dml.py
autonomous: true

must_haves:
  truths:
    - "DML MERGE statements reference source.keyset_id instead of computing keyset inline"
    - "DML without mapping still uses 'architect-generated' fallback"
    - "PostgreSQL INSERT...ON CONFLICT statements alias staging table as source"
    - "All 4 SQL template variants (postgres/tsql x historized/static) use source.keyset_id"
    - "Existing DML tests continue to pass (no regression)"
    - "Generated DML is shorter and cleaner than Phase 8 output"
  artifacts:
    - path: "src/data_architect/generation/dml.py"
      provides: "Simplified _build_metadata_id_expr() returning source.keyset_id, updated SQL templates with source alias"
      contains: "source.keyset_id"
    - path: "tests/test_dml.py"
      provides: "Updated tests verifying keyset_id column reference instead of inline computation"
      contains: "keyset_id"
  key_links:
    - from: "src/data_architect/generation/dml.py"
      to: "staging DDL keyset_id column (from plan 01)"
      via: "_build_metadata_id_expr returns source.keyset_id reference"
      pattern: "source\\.keyset_id"
    - from: "tests/test_dml.py"
      to: "src/data_architect/generation/dml.py"
      via: "Tests verify keyset_id column reference in generated SQL"
      pattern: "keyset_id"
---

<objective>
Simplify all DML MERGE statements to reference the staging table's keyset_id computed column instead of recomputing keyset identity inline.

Purpose: With Plan 01 having added the keyset_id materialized column to staging DDL, DML no longer needs to inline the 148-319 character keyset expression. This simplifies generated SQL, reduces duplication, and ensures the keyset identity is truly computed once (single source of truth).

Output: Simplified _build_metadata_id_expr() in dml.py, updated SQL templates with source alias, updated tests in test_dml.py.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08.1-staging-keyset-single-source-of-truth/08.1-RESEARCH.md
@.planning/phases/08.1-staging-keyset-single-source-of-truth/08.1-01-SUMMARY.md

@src/data_architect/generation/dml.py
@tests/test_dml.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Simplify _build_metadata_id_expr() and update DML SQL templates</name>
  <files>src/data_architect/generation/dml.py</files>
  <action>
**Simplify _build_metadata_id_expr():**

Replace the entire function body with:
```python
def _build_metadata_id_expr(
    anchor: Anchor, mapping: StagingMapping | None, dialect: str
) -> str:
    """Build metadata_id expression: keyset column reference or fallback.

    When a staging mapping is provided, references the pre-computed
    keyset_id column from staging DDL (Phase 08.1 single source of truth).
    When no mapping is provided, returns the literal 'architect-generated'.

    Args:
        anchor: Anchor model instance (unused when mapping provided, kept for API compat)
        mapping: Optional staging mapping
        dialect: Target SQL dialect (unused when mapping provided, kept for API compat)

    Returns:
        SQL expression string for embedding in f-string SQL templates.
    """
    if mapping is None:
        return "'architect-generated'"
    return "source.keyset_id"
```

Remove unused imports that were only needed for inline keyset computation:
- Remove `build_composite_natural_key_expr` from the keyset_sql import (if not used elsewhere)
- Remove `build_keyset_expr` from the keyset_sql import (if not used elsewhere)
- Remove `escape_delimiters` from identity.escaping import (if not used elsewhere)

IMPORTANT: Check if any other function in dml.py uses these imports before removing. If only _build_metadata_id_expr used them, they can be safely removed. The noqa: ARG001 suppression can be removed from the function since dialect and anchor are documented as kept for API compatibility.

**Update PostgreSQL SQL templates to alias staging table as "source":**

Currently, PostgreSQL INSERT...ON CONFLICT templates use unqualified staging table references (e.g., `FROM stg_customers`). The MERGE templates for tsql/snowflake already alias as `source`. For source.keyset_id to work in PostgreSQL templates, the FROM clause needs a source alias.

In `build_anchor_merge()`, PostgreSQL template:
- Change `FROM {source_table}` to `FROM {source_table} AS source`
- Change unqualified column refs in SELECT to use `source.` prefix for consistency:
  - `{identity_col}` in SELECT becomes `source.{identity_col}`

In `build_attribute_merge()`, all 4 PostgreSQL templates (historized and static):
- Change `FROM {source_table}` to `FROM {source_table} AS source`
- In SELECT clauses, prefix staging column references with `source.`:
  - `{anchor_fk}` becomes `source.{anchor_fk}`
  - `{staging_value_col} AS {value_col}` becomes `source.{staging_value_col} AS {value_col}`
  - For historized: `changed_at` in SELECT becomes `source.changed_at`
- In the static postgres `ON CONFLICT ... DO UPDATE SET` clause, the `metadata_id = {metadata_id_sql}` will now correctly resolve as `metadata_id = source.keyset_id` (since EXCLUDED is from the INSERT, but wait -- PostgreSQL ON CONFLICT uses EXCLUDED, not source. For the DO UPDATE SET, metadata_id should use EXCLUDED.keyset_id? No -- EXCLUDED refers to the row proposed for insertion. The source alias isn't available in the DO UPDATE SET clause.)

CRITICAL NUANCE for PostgreSQL ON CONFLICT DO UPDATE SET:
- In PostgreSQL, the `DO UPDATE SET` clause can reference `EXCLUDED.column` (the proposed row) but NOT the source table alias.
- For the static attribute pattern, `metadata_id = {metadata_id_sql}` in DO UPDATE SET currently works because it inlines the expression. With source.keyset_id, this won't work in DO UPDATE SET.
- Solution: In the static postgres template, use `EXCLUDED.metadata_id` in DO UPDATE SET (since metadata_id is being inserted as source.keyset_id in the INSERT, EXCLUDED.metadata_id will have the right value). Wait, metadata_id is computed in the SELECT, not from a table column. Actually, the INSERT SELECT pattern means the selected value goes into metadata_id, so EXCLUDED.metadata_id = the value selected from source.keyset_id.
- Actually simpler: In the static postgres DO UPDATE SET, change `metadata_id = {metadata_id_sql}` to `metadata_id = EXCLUDED.metadata_id`. This works because EXCLUDED.metadata_id contains whatever was computed in the SELECT. This is actually cleaner anyway.
- For the fallback case (no mapping, metadata_id_sql = "'architect-generated'"), EXCLUDED.metadata_id would also be 'architect-generated', so this works universally.
- Apply same pattern to the other DO UPDATE SET fields that reference expressions: metadata_recorded_at and metadata_recorded_by already use CURRENT_TIMESTAMP and literals.

Let me be precise about each template change:

**build_anchor_merge() postgres template:**
```python
sql = f"""
INSERT INTO {target_table} (
    {identity_col},
    metadata_recorded_at,
    metadata_recorded_by,
    metadata_id
)
SELECT
    source.{identity_col},
    CURRENT_TIMESTAMP AS metadata_recorded_at,
    'architect' AS metadata_recorded_by,
    {metadata_id_sql} AS metadata_id
FROM {source_table} AS source
ON CONFLICT ({identity_col}) DO NOTHING
"""
```

**build_anchor_merge() tsql/snowflake template:** Already uses `source` alias. Just verify {metadata_id_sql} resolves to source.keyset_id -- it will because the MERGE USING clause already aliases as source.

**build_attribute_merge() postgres historized template:**
```python
sql = f"""
INSERT INTO {target_table} (
    {anchor_fk},
    {value_col},
    changed_at,
    recorded_at,
    metadata_recorded_at,
    metadata_recorded_by,
    metadata_id
)
SELECT
    source.{anchor_fk},
    source.{staging_value_col} AS {value_col},
    source.changed_at,
    CURRENT_TIMESTAMP AS recorded_at,
    CURRENT_TIMESTAMP AS metadata_recorded_at,
    'architect' AS metadata_recorded_by,
    {metadata_id_sql} AS metadata_id
FROM {source_table} AS source
ON CONFLICT ({anchor_fk}, changed_at) DO NOTHING
"""
```

**build_attribute_merge() postgres static template:**
```python
sql = f"""
INSERT INTO {target_table} (
    {anchor_fk},
    {value_col},
    metadata_recorded_at,
    metadata_recorded_by,
    metadata_id
)
SELECT
    source.{anchor_fk},
    source.{staging_value_col} AS {value_col},
    CURRENT_TIMESTAMP AS metadata_recorded_at,
    'architect' AS metadata_recorded_by,
    {metadata_id_sql} AS metadata_id
FROM {source_table} AS source
ON CONFLICT ({anchor_fk}) DO UPDATE SET
    {value_col} = EXCLUDED.{value_col},
    metadata_recorded_at = CURRENT_TIMESTAMP,
    metadata_recorded_by = 'architect',
    metadata_id = EXCLUDED.metadata_id
"""
```
Note: Changed the DO UPDATE SET to use EXCLUDED references instead of repeating expressions. This is cleaner and works with both keyset_id and fallback.

**build_attribute_merge() tsql/snowflake templates:** Already alias as source. Just verify source.{staging_value_col} and {metadata_id_sql} work. The tsql historized template already uses `source.{anchor_fk}`, `source.{staging_value_col}`, etc. The tsql static template already uses these. Both should work since {metadata_id_sql} = "source.keyset_id".

After all changes, verify the removed imports don't break anything with `make check`.
  </action>
  <verify>Run `cd /workspaces/data-architect && python -m pytest tests/test_dml.py -x -v` -- existing tests should still pass. Some keyset-specific tests (test_build_anchor_merge_with_mapping_has_keyset etc.) may need updating in Task 2 since they check for inline keyset patterns. Run `make check` for full validation.</verify>
  <done>_build_metadata_id_expr() simplified, PostgreSQL templates aliased as source, unused keyset imports removed, DML generates source.keyset_id references.</done>
</task>

<task type="auto">
  <name>Task 2: Update DML tests for keyset_id column reference pattern</name>
  <files>tests/test_dml.py</files>
  <action>
Update the Phase 8 keyset identity tests to verify the new source.keyset_id pattern instead of inline computation:

1. **test_build_anchor_merge_with_mapping_has_keyset** -- Change assertions:
   - Remove: assert "Customer@Northwind~ACME" in sql (no longer inline)
   - Remove: assert "CASE" in sql.upper() (no longer inline)
   - Remove: assert "REPLACE" in sql.upper() (no longer inline)
   - Add: assert "keyset_id" in sql (references computed column)
   - Add: assert "source.keyset_id" in sql or "source" in sql (source alias used)
   - Keep: assert "'architect-generated'" not in sql (still correct)

2. **test_build_anchor_merge_without_mapping_has_fallback** -- Keep unchanged (still correct).

3. **test_build_attribute_merge_with_mapping_has_keyset** -- Change:
   - Remove: assert "Customer@ERP~US" in sql
   - Add: assert "keyset_id" in sql

4. **test_build_attribute_merge_column_mapping_used** -- May need update. The source.{staging_value_col} pattern should still work. Verify "first_name_source" still appears in SQL.

5. **test_build_anchor_merge_composite_natural_key** -- Change:
   - Remove: assert "Customer@SAP~EU" in sql
   - Remove: assert "CONCAT" in sql.upper() (no longer in DML)
   - Remove: assert "CASE" in sql.upper() (no longer in DML)
   - Add: assert "keyset_id" in sql

6. **test_generate_all_dml_multi_source_has_keyset** -- Change:
   - Remove: assert "Customer@Northwind" in nw_sql
   - Remove: assert "Customer@SAP" in sap_sql
   - Add: assert "keyset_id" in nw_sql
   - Add: assert "keyset_id" in sap_sql
   - Keep: assert "'architect-generated'" not in nw_sql / sap_sql

7. Add new test **test_build_anchor_merge_postgres_has_source_alias** -- Verify PostgreSQL template uses "AS source" in FROM clause.

8. Add new test **test_build_attribute_merge_static_uses_excluded** -- For static attribute with postgres, verify DO UPDATE SET uses "EXCLUDED" instead of recomputing expressions.

9. Add new test **test_dml_keyset_reference_is_shorter** -- Generate DML with mapping for postgres, assert the SQL length is significantly shorter than a threshold (e.g., the old inline expression was 148+ chars for metadata_id alone; now it's 15 chars). This proves the simplification worked. Use len(sql) < some reasonable bound, or just assert "source.keyset_id" appears exactly once per metadata_id reference.

Run full test suite to ensure no regressions.

Commit: `feat(08.1-02): simplify DML to reference staging keyset_id column`
  </action>
  <verify>Run `cd /workspaces/data-architect && make check` -- all lint, type checks, and tests pass. Run `python -m pytest tests/ -v --tb=short` to see full test results.</verify>
  <done>DML tests updated to verify source.keyset_id pattern, new tests for source alias and EXCLUDED pattern, all tests pass, make check passes.</done>
</task>

</tasks>

<verification>
1. `make check` -- lint + type + full test suite pass
2. `python -m pytest tests/test_ddl.py tests/test_dml.py -v` -- all DDL and DML tests pass
3. Generate SQL for a spec with staging_mappings:
   - DDL: staging table has keyset_id GENERATED ALWAYS AS (...) STORED column
   - DML: MERGE references source.keyset_id (no inline keyset computation)
4. Generate SQL without staging_mappings:
   - DDL: no keyset_id column (backward compat)
   - DML: metadata_id = 'architect-generated' (backward compat)
</verification>

<success_criteria>
- _build_metadata_id_expr() returns "source.keyset_id" when mapping provided (not inline computation)
- _build_metadata_id_expr() returns "'architect-generated'" when no mapping (backward compat)
- PostgreSQL templates use "AS source" alias in FROM clause
- Static attribute DO UPDATE SET uses EXCLUDED references
- All existing DML tests pass (updated assertions match new pattern)
- No unused imports remain in dml.py
- Generated DML is significantly shorter and cleaner
- make check passes
</success_criteria>

<output>
After completion, create `.planning/phases/08.1-staging-keyset-single-source-of-truth/08.1-02-SUMMARY.md`
</output>
