---
phase: 08.1-staging-keyset-single-source-of-truth
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - src/data_architect/generation/columns.py
  - src/data_architect/generation/ddl.py
  - tests/test_ddl.py
autonomous: true

must_haves:
  truths:
    - "Staging table DDL contains a keyset_id computed column when anchor+mapping context is provided"
    - "Keyset_id expression in DDL matches the existing keyset format (entity@system~tenant|natural_key)"
    - "Composite natural keys produce correct keyset_id computed column with NULL propagation"
    - "Staging DDL without mapping context (backward compat) has no keyset_id column"
    - "Keyset_id column works across postgres, tsql, and snowflake dialects"
  artifacts:
    - path: "src/data_architect/generation/columns.py"
      provides: "build_keyset_column() function returning sge.ColumnDef with ComputedColumnConstraint"
      contains: "build_keyset_column"
    - path: "src/data_architect/generation/ddl.py"
      provides: "Updated build_staging_table() accepting anchor+mapping, generate_all_ddl() passing anchor context"
      contains: "build_keyset_column"
    - path: "tests/test_ddl.py"
      provides: "Tests for keyset_id computed column in staging DDL"
      contains: "keyset_id"
  key_links:
    - from: "src/data_architect/generation/columns.py"
      to: "src/data_architect/generation/keyset_sql.py"
      via: "build_keyset_column() calls build_keyset_expr() and build_composite_natural_key_expr()"
      pattern: "build_keyset_expr|build_composite_natural_key_expr"
    - from: "src/data_architect/generation/ddl.py"
      to: "src/data_architect/generation/columns.py"
      via: "build_staging_table() calls build_keyset_column()"
      pattern: "build_keyset_column"
    - from: "src/data_architect/generation/ddl.py"
      to: "generate_all_ddl() passes anchor+mapping to build_staging_table()"
      via: "generate_all_ddl staging section"
      pattern: "anchor.*mapping"
---

<objective>
Add a materialized keyset_id computed column to staging table DDL so keyset identity is computed once at load time (single source of truth).

Purpose: Currently keyset identity is computed inline in every DML MERGE statement (148-319 chars per occurrence). Moving it to a GENERATED ALWAYS AS (...) STORED column in staging DDL eliminates duplication, simplifies downstream DML, and ensures a single source of truth for keyset computation.

Output: build_keyset_column() helper in columns.py, updated build_staging_table() and generate_all_ddl() in ddl.py, TDD tests in test_ddl.py.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08.1-staging-keyset-single-source-of-truth/08.1-RESEARCH.md

@src/data_architect/generation/columns.py
@src/data_architect/generation/ddl.py
@src/data_architect/generation/keyset_sql.py
@src/data_architect/generation/naming.py
@src/data_architect/models/anchor.py
@src/data_architect/models/staging.py
@src/data_architect/identity/escaping.py
@tests/test_ddl.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: TDD RED -- Write failing tests for keyset_id computed column in staging DDL</name>
  <files>tests/test_ddl.py</files>
  <action>
Add new test section "Keyset Computed Column Tests" to test_ddl.py after existing staging DDL tests. Write the following failing tests:

1. **test_build_keyset_column_single_key** -- Call build_keyset_column(anchor, mapping, "postgres") with single natural_key_columns=["CustomerID"]. Assert returns sge.ColumnDef. Assert generated SQL contains "keyset_id", "VARCHAR(500)", "GENERATED ALWAYS AS", "STORED", and the keyset prefix "Customer@ERP~ACME".

2. **test_build_keyset_column_composite_key** -- Use mapping with natural_key_columns=["MANDT", "KUNNR"]. Assert SQL contains composite key handling: CONCAT, ":", NULL propagation CASE, and the keyset prefix.

3. **test_build_keyset_column_multi_dialect** -- For each dialect in ["postgres", "tsql", "snowflake"], call build_keyset_column and generate SQL. Assert postgres has "GENERATED ALWAYS AS ... STORED", tsql has "AS ... PERSISTED". All should contain "keyset_id".

4. **test_build_staging_table_with_keyset_column** -- Call build_staging_table("stg_customers", columns, "postgres", anchor=anchor, mapping=mapping). Assert resulting SQL contains "keyset_id" and "GENERATED ALWAYS AS".

5. **test_build_staging_table_without_anchor_no_keyset** -- Call build_staging_table("stg_test", columns, "postgres") with no anchor/mapping args. Assert SQL does NOT contain "keyset_id". This verifies backward compatibility.

6. **test_generate_all_ddl_staging_has_keyset_column** -- Create Spec with anchor having staging_mappings. Call generate_all_ddl(spec, "postgres"). Assert staging table SQL output contains "keyset_id".

Import build_keyset_column from data_architect.generation.columns at top of file.

Use the existing test fixture patterns: Anchor(mnemonic="CU", descriptor="Customer", identity="bigint") and StagingMapping with system/tenant/table/natural_key_columns/columns.

Commit with --no-verify: `test(08.1-01): add failing tests for keyset_id computed column in staging DDL`
  </action>
  <verify>Run `cd /workspaces/data-architect && python -m pytest tests/test_ddl.py -x -v 2>&1 | tail -20` -- tests should FAIL (ImportError or AssertionError since build_keyset_column doesn't exist yet).</verify>
  <done>6 new failing tests committed with --no-verify. Tests cover single key, composite key, multi-dialect, staging table integration, backward compat, and generate_all_ddl integration.</done>
</task>

<task type="auto">
  <name>Task 2: TDD GREEN -- Implement build_keyset_column() and update staging DDL builders</name>
  <files>src/data_architect/generation/columns.py, src/data_architect/generation/ddl.py</files>
  <action>
**In columns.py:**

Add `build_keyset_column(anchor, mapping, dialect)` function that:

1. Import `build_keyset_expr` and `build_composite_natural_key_expr` from keyset_sql.py, `escape_delimiters` from identity.escaping, `Anchor` from models.anchor, `StagingMapping` from models.staging.
2. Use TYPE_CHECKING guard for Anchor and StagingMapping to avoid circular imports (since columns.py is imported by ddl.py which imports models). Actually, check existing import patterns in ddl.py -- if ddl.py already imports these models directly, columns.py can too since it's a leaf module called by ddl.py.
3. For single natural key (len(mapping.natural_key_columns) == 1):
   - Call `build_keyset_expr(anchor.descriptor, mapping.system, mapping.tenant, mapping.natural_key_columns[0], dialect)` to get the expression AST.
4. For composite natural keys (len > 1):
   - Replicate the composite key logic from dml.py `_build_metadata_id_expr()` lines 64-92, but return an sge.Expression (NOT a string). Build:
     - `composite_nk_expr = build_composite_natural_key_expr(mapping.natural_key_columns, dialect)`
     - Escape prefix components: `escape_delimiters(anchor.descriptor)`, etc.
     - Build prefix literal string
     - Build nested REPLACE on CAST of composite expression
     - Wrap in CASE WHEN composite IS NULL THEN NULL ELSE CONCAT(prefix, escaped) END
     - IMPORTANT: Build this as SQLGlot AST nodes (sge.Case, sge.Concat, sge.Replace, sge.Cast, sge.Literal), NOT as f-string SQL. The ComputedColumnConstraint requires an sge.Expression, not a string. However, if AST construction is too complex, use `sg.parse_one()` to parse a SQL string into an AST node -- this is acceptable since dml.py already uses this pattern.
5. Return `sge.ColumnDef` with:
   - `this=sg.to_identifier("keyset_id")`
   - `kind=sge.DataType.build("VARCHAR(500)", dialect=dialect)`
   - `constraints=[sge.ColumnConstraint(kind=sge.ComputedColumnConstraint(this=keyset_expr, persisted=True))]`

**In ddl.py:**

1. Add import: `from data_architect.generation.columns import build_keyset_column`
2. Update `build_staging_table()` signature to accept optional `anchor` and `mapping` parameters:
   ```python
   def build_staging_table(
       name: str,
       columns: list[tuple[str, str]],
       dialect: str,
       anchor: Anchor | None = None,
       mapping: StagingMapping | None = None,
   ) -> sge.Create:
   ```
   Add `StagingMapping` to the imports from models.staging if not already present.
3. Between user-defined columns and metadata columns, add:
   ```python
   # 2. Keyset computed column (when anchor context available)
   if anchor is not None and mapping is not None:
       column_defs.append(build_keyset_column(anchor, mapping, dialect))
   ```
4. Update `generate_all_ddl()` staging section:
   - Change `staging_tables` dict type to store `tuple[str, Anchor, StagingMapping, list[tuple[str, str]]]`
   - In the loop: `staging_tables[table] = (table, anchor, mapping, columns)`
   - In the generation loop: `name, anchor_ref, mapping_ref, columns = staging_tables[table]`
   - Pass to build_staging_table: `build_staging_table(name, columns, dialect, anchor=anchor_ref, mapping=mapping_ref)`

Run tests. If any fail, debug and fix. Common issues:
- SQLGlot ComputedColumnConstraint may need the expression to be parseable. If direct AST construction fails, use `sg.parse_one(keyset_sql_string, dialect=dialect)` to convert the SQL string to an AST.
- Type annotations: use `from __future__ import annotations` if needed for forward refs.

Commit: `feat(08.1-01): implement keyset_id computed column in staging DDL`
  </action>
  <verify>Run `cd /workspaces/data-architect && python -m pytest tests/test_ddl.py -x -v` -- ALL tests pass (existing + new). Run `make check` to verify lint, type, and full test suite pass.</verify>
  <done>build_keyset_column() implemented in columns.py, build_staging_table() updated to accept anchor/mapping, generate_all_ddl() passes anchor context, all DDL tests pass, make check passes.</done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/test_ddl.py -v` -- all tests pass
2. `make check` -- lint, type, and full test suite pass
3. Manual verification: generate staging DDL for a spec with staging_mappings and confirm keyset_id column appears with correct GENERATED ALWAYS AS expression
</verification>

<success_criteria>
- build_keyset_column() exists in columns.py and returns correct ColumnDef with ComputedColumnConstraint
- build_staging_table() accepts optional anchor/mapping and adds keyset_id column when provided
- generate_all_ddl() passes anchor context to staging DDL builder
- Backward compatibility: calling build_staging_table without anchor/mapping produces same output as before
- All 3 dialects (postgres, tsql, snowflake) generate valid computed column syntax
- Composite natural keys handled with NULL propagation
- make check passes (lint + type + test)
</success_criteria>

<output>
After completion, create `.planning/phases/08.1-staging-keyset-single-source-of-truth/08.1-01-SUMMARY.md`
</output>
