---
phase: 04-agent-quality-modeling-workflows
plan: 03
type: execute
wave: 3
depends_on: ["04-02"]
files_modified:
  - src/data_architect/templates.py
  - tests/test_scaffold.py
autonomous: true

must_haves:
  truths:
    - "da-start skill encodes source document discovery protocol so agents can read source schemas from filesystem"
    - "da-start skill guides business question gathering as input to CLP debate"
    - "da-export skill encodes YAML spec validation checks before export"
    - "da-review skill references the expanded anti-pattern checklist and convergence detection"
    - "Example YAML specs are expanded with comments showing methodology reasoning"
  artifacts:
    - path: "src/data_architect/templates.py"
      provides: "Deepened skill and spec example template strings"
      contains: "source_documents"
    - path: "tests/test_scaffold.py"
      provides: "Tests verifying skill depth and spec format"
      contains: "da_start_has_source_discovery"
  key_links:
    - from: "da-start skill"
      to: "data-architect.md CLP protocol"
      via: "Skill gathers inputs (business questions, source docs) that CLP protocol consumes"
      pattern: "source.*schema|business.*question"
    - from: "da-export skill"
      to: "YAML spec examples"
      via: "Export validates against spec format shown in examples"
      pattern: "domain-example|anchor-example"
    - from: "da-review skill"
      to: "veteran-reviewer.md anti-pattern checklist"
      via: "Review skill triggers anti-pattern detection"
      pattern: "anti-pattern|checklist"
---

<objective>
Deepen the four skill definitions (da-start, da-review, da-status, da-export) with modeling workflow protocols and expand the YAML spec examples to serve as output format references for CLP debate output.

Purpose: Skills are user entry points to the agent team. The current skills say WHAT to do but lack the depth to guide source document discovery, business question gathering, structured debate output, and spec validation. This plan makes skills methodology-aware so users get genuine modeling workflows, not generic prompts. The expanded examples ensure agents produce consistent, methodology-compliant YAML output.

Output: Updated skill and spec example template strings in templates.py with modeling workflow depth, plus tests verifying the content.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-agent-quality-modeling-workflows/04-RESEARCH.md
@.planning/phases/04-agent-quality-modeling-workflows/04-01-SUMMARY.md
@.planning/phases/04-agent-quality-modeling-workflows/04-02-SUMMARY.md
@src/data_architect/templates.py
@tests/test_scaffold.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Deepen skills with modeling workflow protocols</name>
  <files>src/data_architect/templates.py, tests/test_scaffold.py</files>
  <action>
Update all four skill entries in TEMPLATES. Keep ALL existing content for each skill and ADD depth.

**da-start skill** (".opencode/skills/da-start/SKILL.md"):

After the existing "4. Ask initial discovery questions" step, ADD a new section "## Source Document Discovery" with protocol:

1. Ask user to point to source system documentation in the filesystem:
   - Swagger/OpenAPI files (.json, .yaml): "Where are your API specs?"
   - OData metadata ($metadata): "Where are your OData service definitions?"
   - ERD/database schemas (.sql, .dbml): "Where are your database schema files?"
   - Business requirement documents (.md, .docx, .pdf): "Where is your business context documented?"
2. For each file path provided, read and catalog it for @system-analyst to analyze
3. If no source documentation available, note that debate will rely more heavily on user-provided business context

Also ADD a section "## Business Question Gathering" with protocol:
1. Ask the user: "What are the top 3-5 business questions this warehouse must answer?"
2. For each question, identify: What entities are involved? What time periods matter? What aggregations are needed?
3. Document these questions as debate evidence for @business-analyst
4. Examples of good business questions: "What was our revenue by product category last quarter?", "How has customer churn changed month over month?", "Which suppliers have the longest delivery times?"

Also ADD to the existing "## Remember" section: "Start every session by gathering BOTH source documentation AND business questions -- these are the two inputs that drive the entire CLP debate."

---

**da-review skill** (".opencode/skills/da-review/SKILL.md"):

After the existing "5. Perform anti-pattern detection" step, ADD detail:

Replace the current bullet list in step 5 with a more explicit protocol:
"Invoke your review process using the full 10-item anti-pattern checklist. For each anti-pattern, apply the detection criteria to the spec. Report findings with:
- The anti-pattern name and number
- The specific element(s) that triggered detection (anchor name, attribute name, tie name)
- The detection criteria that matched
- The recommended fix from the fix template"

Also ADD after step 6 a new step "7. **Validate YAML structure**: Confirm the spec follows the structure in `.data-architect/specs/examples/domain-example.yaml`:
- Domain has name and description
- Each anchor has name, description, and attributes list
- Each attribute has name, data_type, and historized flag
- Each tie has name, description, from_anchor, to_anchor, and historized flag
- Each knot has name, description, data_type
- Each nexus has name, description, anchors list
- All names follow naming conventions (snake_case, double underscore separators, no mnemonics)"

---

**da-status skill** (".opencode/skills/da-status/SKILL.md"):

After the existing "3. Summarize open questions" step, ADD to the open questions summary:
"For each unresolved debate, report:
- The topic under debate (entity classification, relationship direction, historization decision, etc.)
- The System Analyst position and evidence
- The Business Analyst position and evidence
- Current round number (out of 5 maximum)
- Whether convergence, divergence, or stagnation is detected"

---

**da-export skill** (".opencode/skills/da-export/SKILL.md"):

After the existing "2. Validate completeness" step, ADD a new step "3. **Validate methodology compliance** before export:
- All anchors follow anchor__<entity> naming
- All attributes follow anchor__<entity>__<attribute> naming
- All ties follow tie__<from>__<verb>__<to> naming
- All knots follow knot__<name> naming
- All nexuses follow nexus__<name> naming
- No mnemonics (abbreviations) in any names
- Every attribute has historized flag set (no missing flags)
- Every tie has from_anchor and to_anchor referencing defined anchors
- The spec has passed @veteran-reviewer's checklist (warn if not reviewed)"

Renumber subsequent steps accordingly (current step 3 becomes 4, etc.)

Also ADD to the Remember section: "Never export a spec that has not been reviewed by @veteran-reviewer. If the user requests export before review, warn them and offer to run a review first."

---

**Add tests** in tests/test_scaffold.py:

1. `test_da_start_has_source_discovery()`: Assert da-start skill template contains "Source Document Discovery" and "Swagger" and "business question" (case-insensitive)
2. `test_da_review_has_yaml_validation()`: Assert da-review skill template contains "Validate YAML" or "YAML structure" and "naming conventions"
3. `test_da_export_has_methodology_compliance()`: Assert da-export skill template contains "methodology compliance" or "naming" and "veteran-reviewer"

Important: Do NOT change the template count (14 entries). Only modify existing template string content.
  </action>
  <verify>
Run `cd /workspaces/data-architect && make check` -- all tests pass. Template count still 14. All four skills have deeper workflow protocols. New tests pass.
  </verify>
  <done>
All four skills deepened: da-start has source document discovery protocol and business question gathering. da-review has explicit anti-pattern checklist invocation and YAML structure validation. da-status reports debate convergence state. da-export has pre-export methodology compliance validation and veteran-reviewer gate. Three new tests verify the deepened content.
  </done>
</task>

<task type="auto">
  <name>Task 2: Expand YAML spec examples with methodology reasoning comments</name>
  <files>src/data_architect/templates.py, tests/test_scaffold.py</files>
  <action>
Update the two YAML spec example entries in TEMPLATES to serve as comprehensive format references for CLP debate output.

**anchor-example.yaml** (".data-architect/specs/examples/anchor-example.yaml"):

Keep the existing structure but ADD methodology reasoning as YAML comments. Expand to include:
- A comment block at top explaining what this file demonstrates and when to use it as a reference
- For the historized: true attributes, add a comment explaining WHY (e.g., "# Customer name can change (marriage, legal change) -- historize to track")
- For the historized: false attributes, add a comment explaining WHY (e.g., "# Birth date is immutable by definition -- no history needed")
- Add a comment block at bottom summarizing: "This anchor has 4 attributes. If an anchor grows beyond 15 attributes, check for God Anchor anti-pattern."
- Add a section showing an attribute that references a knot: e.g., anchor__customer__preferred_currency with a comment "# References knot__currency -- shared reference data, not inline value"

**domain-example.yaml** (".data-architect/specs/examples/domain-example.yaml"):

Keep the existing structure but ADD methodology reasoning. Expand to include:
- A comment block at top explaining this is the reference format for complete domain specs produced by CLP debate
- For each anchor, add a comment showing the Anchor vs Attribute Decision Tree result: "# Customer has independent identity (Step 1: YES), exists without other entities (Step 2: YES), business asks 'how many customers?' (Step 3: YES) -> ANCHOR"
- For each tie, add a comment explaining the many-to-one direction: "# Many orders are placed by one customer. Direction: order (many) -> customer (one)"
- For historized ties, add reasoning: "# Product can be recategorized over time -- historize to track category changes"
- For knots, add reasoning: "# Currency is used by orders AND products -- shared reference data -> KNOT"
- For the nexus, add reasoning: "# Order lines connect orders and products many-to-many -- neither side is 'one' -> NEXUS"
- Add a metadata section at the bottom as YAML comments showing: domain name, number of anchors, ties, knots, nexuses, and a note "# Generated by CLP debate. Reviewed by @veteran-reviewer."

**Add tests** in tests/test_scaffold.py:

1. `test_anchor_example_has_methodology_reasoning()`: Assert anchor-example.yaml template contains "immutable" and "historize" and "anti-pattern" (methodology reasoning is present)
2. `test_domain_example_has_decision_tree_reasoning()`: Assert domain-example.yaml template contains "Decision Tree" or "independent identity" and "many-to-one" (methodology reasoning applied to examples)

Important: Do NOT change the template count (14 entries). Only modify existing template string content. The examples still produce valid YAML -- comments are YAML-legal.
  </action>
  <verify>
Run `cd /workspaces/data-architect && make check` -- all tests pass. Template count still 14. YAML examples contain methodology reasoning comments. New tests pass.
  </verify>
  <done>
Both YAML spec examples expanded with methodology reasoning as YAML comments: anchor-example shows historization reasoning, knot references, and anti-pattern threshold warnings; domain-example shows Decision Tree application, tie direction reasoning, knot justification, and nexus classification reasoning. Two new tests verify methodology reasoning is present. All existing tests still pass. Template count unchanged at 14.
  </done>
</task>

</tasks>

<verification>
Phase-level checks after this plan (and final Phase 4 verification):
1. `make check` passes (lint, type, test)
2. TEMPLATES dict still has 14 entries
3. All four skills have deep workflow protocols (source discovery, evidence gathering, anti-pattern detection, methodology compliance)
4. Both YAML spec examples have methodology reasoning comments
5. All tests pass (original 33+ from Phase 3, plus ~15 new tests added across all 3 plans)

FULL PHASE 4 VERIFICATION (after all 3 plans):
- QUAL-01: AGENTS.md has decision trees (anchor vs attribute, historization, tie vs nexus, knot) -- verified in 04-01 tests
- QUAL-02: Data Architect has CLP stage protocols (Conceptual, Logical, Physical) -- verified in 04-01 tests
- QUAL-03: Data Architect has debate termination (5 rounds, convergence, escalation) -- verified in 04-01 tests
- QUAL-04: SA has source fidelity + burden-of-proof, BA has business meaning + burden-of-proof -- verified in 04-02 tests
- QUAL-05: Veteran Reviewer has 10 anti-patterns with detection criteria + fix templates -- verified in 04-02 tests
- MODL-01: da-start gathers business description and business questions -- verified in 04-03 tests
- MODL-02: da-start gathers source schema paths, SA has reading protocols -- verified in 04-02 and 04-03 tests
- MODL-03: BA has business question evidence templates, da-start gathers questions -- verified in 04-02 and 04-03 tests
- MODL-04: da-export validates and produces YAML specs, examples show correct format -- verified in 04-03 tests
</verification>

<success_criteria>
- All four skills deepened with workflow-specific protocols
- Both YAML spec examples enriched with methodology reasoning comments
- 5+ new tests verify the deepened content
- All prior tests still pass (original + plans 01 and 02)
- Template count unchanged at 14
- ALL Phase 4 requirements (QUAL-01 through QUAL-05, MODL-01 through MODL-04) are addressed
</success_criteria>

<output>
After completion, create `.planning/phases/04-agent-quality-modeling-workflows/04-03-SUMMARY.md`
</output>
