---
phase: 04-agent-quality-modeling-workflows
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - src/data_architect/templates.py
  - tests/test_scaffold.py
autonomous: true

must_haves:
  truths:
    - "System Analyst agent has explicit source schema reading protocols for Swagger, OData, and ERD files with evidence-based argumentation templates"
    - "Business Analyst agent has business question evidence templates and opposing position enforcement"
    - "Veteran Reviewer has expanded anti-pattern checklist with detection criteria, example violations, and fix templates for each of the 10 items"
    - "Data Engineer and Analytics Engineer agents have deepened methodology-specific guidance"
  artifacts:
    - path: "src/data_architect/templates.py"
      provides: "Deepened SA, BA, VR, DE, AE agent template strings"
      contains: "Detection criteria"
    - path: "tests/test_scaffold.py"
      provides: "Tests verifying deepened subagent content"
      contains: "source_schema_reading"
  key_links:
    - from: "system-analyst.md template"
      to: "business-analyst.md template"
      via: "Opposing positions with burden-of-proof rules"
      pattern: "burden of proof|Your burden"
    - from: "veteran-reviewer.md template"
      to: "Anti-pattern detection"
      via: "Detection criteria + fix templates for each anti-pattern"
      pattern: "Detection criteria|Fix template"
---

<objective>
Deepen the five subagent prompts (System Analyst, Business Analyst, Veteran Reviewer, Data Engineer, Analytics Engineer) with methodology-specific depth: source schema reading protocols, opposing position enforcement with burden-of-proof, expanded anti-pattern checklist with detection criteria and fix templates, and consumption validation protocols.

Purpose: Without deep methodology content in subagent prompts, the debate will be shallow -- agents will agree too quickly (premature consensus) or argue without evidence. This plan gives each subagent the specific tools, templates, and rules they need to participate in genuine CLP debate.

Output: Updated subagent template strings in templates.py with deep methodology content, plus tests verifying the content.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-agent-quality-modeling-workflows/04-RESEARCH.md
@.planning/phases/04-agent-quality-modeling-workflows/04-01-SUMMARY.md
@src/data_architect/templates.py
@tests/test_scaffold.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Deepen System Analyst and Business Analyst with opposing positions and evidence protocols</name>
  <files>src/data_architect/templates.py, tests/test_scaffold.py</files>
  <action>
Update the ".opencode/agents/system-analyst.md" entry in TEMPLATES. Keep ALL existing content and ADD:

**Add section "## Source Schema Reading Protocol"** (after "## When Invoked"):

For Swagger/OpenAPI files (.json, .yaml):
1. Read file from filesystem using available tools
2. Extract entities: Each schema definition under `components.schemas` is a candidate entity
3. Extract attributes: Properties within each schema
4. Extract relationships: Look for `$ref` references between schemas (FK equivalents)
5. Note constraints: Required fields, nullable fields, enum values
6. Present findings in structured format showing entities, relationships, constraints

For OData Metadata ($metadata):
1. Read XML metadata document
2. Extract EntityTypes from `<EntityType>` elements
3. Extract Properties from `<Property>` elements
4. Extract Associations from `<NavigationProperty>` elements
5. Present findings in same structured format

For ERD/Database Schemas (.sql, .dbml):
1. Read schema definition
2. Extract tables (CREATE TABLE = candidate entity)
3. Extract columns (column definitions = candidate attributes)
4. Extract foreign keys (FK constraints = relationships)
5. Present findings in same structured format

**Add section "## Evidence-Based Argumentation"** (after source schema reading):

When debating with @business-analyst, always cite specific source evidence:
- GOOD example: "The source schema shows Order.customer_id as a required FK to Customer.id, indicating every order MUST have a customer. This supports modeling as tie__order__placed_by__customer."
- BAD example: "I think orders are probably related to customers."
- Always provide file paths and specific properties when citing source evidence.

**Your Burden of Proof:** When arguing against business needs, you must show technical impossibilities or data absence -- "the source doesn't contain X" or "the FK constraint prevents Y." Opinion alone is insufficient.

**Challenge @business-analyst when:**
- They propose entities not present in source schemas
- They want to model relationships differently than source FKs
- They ignore technical constraints (nullability, uniqueness)
- They rename concepts without preserving traceability to source semantics

---

Update the ".opencode/agents/business-analyst.md" entry in TEMPLATES. Keep ALL existing content and ADD:

**Add section "## Business Question Evidence Templates"** (after "## When Invoked"):

When arguing for design decisions, business questions are your strongest evidence. Structure arguments using this template:
"The business needs to answer: [question]. This requires [anchor/attribute/tie] because [reasoning]."

Include 2-3 good and bad examples:
- GOOD: "The business needs to answer: 'What was the customer's loyalty tier on the date of this order?' This requires historized attribute anchor__customer__loyalty_tier because the tier may have changed since the order was placed."
- GOOD: "The business asks: 'How many orders did each sales rep close last quarter?' This requires tie__order__closed_by__sales_rep, not an attribute, because sales rep is an independent entity."
- BAD: "I think customer tier should be historized."
- BAD: "Sales rep is probably a relationship."

**Add section "## Gathering Business Questions"**:
1. Ask user directly: "What business questions does this warehouse need to answer?"
2. Infer from business description: If user describes "monthly sales reports by region," infer "What were sales by region for month X?"
3. Challenge assumptions: If @system-analyst proposes design, ask: "What business question does this answer?"
Document all business questions in session context. Reference them during debate.

**Your Burden of Proof:** When arguing against source structure, you must show business evidence that outweighs technical reality -- "the business treats X as Y because [specific business question or process]." Preference alone is insufficient.

**Challenge @system-analyst when:**
- They mirror source structure without business justification
- They reject historization based only on source system limitations
- They use technical jargon instead of business terms
- They model relationships that don't match how the business understands the domain

---

**Add tests** in tests/test_scaffold.py:

1. `test_system_analyst_has_source_schema_protocol()`: Assert system-analyst.md template contains "Source Schema Reading", "Swagger", "OData", "ERD"
2. `test_business_analyst_has_evidence_templates()`: Assert business-analyst.md template contains "Business Question Evidence", "business needs to answer"
3. `test_analysts_have_burden_of_proof()`: Assert BOTH system-analyst.md and business-analyst.md contain "burden" or "Burden" (case-insensitive check via lowering the string)

Important: Do NOT change the template count (14 entries). Only modify existing template string content.
  </action>
  <verify>
Run `cd /workspaces/data-architect && make check` -- all tests pass. Template count still 14. System Analyst has source schema reading protocols. Business Analyst has evidence templates. Both have burden-of-proof rules.
  </verify>
  <done>
System Analyst agent has complete source schema reading protocols for Swagger/OData/ERD with evidence-based argumentation rules. Business Analyst has business question evidence templates with structured argumentation format. Both agents have explicit burden-of-proof requirements and challenge criteria for opposing their counterpart. Three new tests verify the deepened content.
  </done>
</task>

<task type="auto">
  <name>Task 2: Expand Veteran Reviewer anti-pattern checklist and deepen Data Engineer + Analytics Engineer</name>
  <files>src/data_architect/templates.py, tests/test_scaffold.py</files>
  <action>
Update the ".opencode/agents/veteran-reviewer.md" entry in TEMPLATES. The existing 10-item anti-pattern checklist is already there (from Phase 3). Expand EACH of the 10 items to include three additional subsections: **Detection criteria** (specific measurable signals), **Example violation** (concrete YAML showing the problem), and **Fix template** (step-by-step remediation).

For each of the 10 anti-patterns, add under the existing "Fix" line:

**1. God Anchor** - Add detection criteria: "Anchor has 15+ attributes, attribute names span multiple business concepts, attributes change at different rates." Add example YAML violation showing anchor__customer with 20+ attributes spanning billing, shipping, loyalty. Add fix template: "Group attributes by change rate and business concept. Extract each group into separate anchor. Create ties to maintain relationships. Validate with @business-analyst."

**2. Missing Historization** - Add detection criteria: "Attribute marked historized: false, but @business-analyst mentioned time-based questions about it, or source shows update timestamps, or business process involves correcting/updating this value." Add fix: "Change historized: false to historized: true. Add validity period columns in physical schema. Document business justification."

**3. Circular Ties** - Add detection criteria: "Trace all tie chains -- if following from_anchor to to_anchor leads back to a previously visited anchor, you have a circle." Add fix: "Draw the cycle on paper. Identify which tie represents a temporal state vs a permanent relationship. Break the cycle by collapsing one leg or reversing direction."

**4. Tie Masquerading as Anchor** - Add detection criteria: "Anchor name contains two other entity names (e.g., anchor__customer_order_assignment). Anchor has exactly two ties going out, one to each named entity. Anchor has no attributes of its own besides the relationship metadata." Add fix: "Replace with tie__<from>__<verb>__<to>. Move relationship metadata to tie attributes if needed."

**5. Orphan Anchors** - Add detection criteria: "Anchor has zero ties to any other anchor. Not referenced by any nexus. No relationship path connects it to the rest of the model." Add fix: "Identify missing relationships. If the anchor truly stands alone, question whether it belongs in this domain spec or is a candidate for a separate domain."

**6. Knot Overuse** - Add detection criteria: "Knot is referenced by only one anchor. Knot values are entity-specific, not shared reference data. Knot values are large or frequently changing." Add fix: "Convert to attribute of the anchor. Reserve knots for small, stable, shared lookup values."

**7. Naming Violations** - Add detection criteria: "Any name containing abbreviated words (cust, prod, ord, amt). Single underscore where double underscore should be. CamelCase or UPPER_CASE. Tie name not following tie__<from>__<verb>__<to> pattern." Add fix: "Apply naming convention rules from AGENTS.md. Full readable names, snake_case, double underscore separators."

**8. Missing Business Context** - Add detection criteria: "Attribute exists but @business-analyst cannot explain what business question it answers. Attribute was added because 'the source had this field.' No business justification documented." Add fix: "Challenge the attribute. If no business question requires it, consider removing from DAB (keep in DAS only)."

**9. Premature Physical Optimization** - Add detection criteria: "Logical model contains index hints, partition schemes, or materialized view definitions. Entity classification driven by query patterns rather than business meaning. Denormalization in the logical stage." Add fix: "Remove physical concerns from logical model. Document them separately for the Physical stage. Let @data-engineer handle optimization after logical model is stable."

**10. Anchor/Attribute Confusion** - Add detection criteria: "Something modeled as anchor has no ties and is only referenced as a property of another entity. Something modeled as attribute has its own sub-attributes or relationships." Add fix: "Apply the Anchor vs Attribute Decision Tree from AGENTS.md. Identity = anchor. Property = attribute."

Also add a new section **"## Review Protocol"** after the checklist:
- First review: After Logical stage, focus on items 1-5 and 7-8 (structural issues)
- Final review: After Physical stage, full 10-item checklist with emphasis on item 9 (premature optimization)
- For each issue found: cite the specific anchor/attribute/tie name, the anti-pattern number, and the specific fix recommendation

---

Update ".opencode/agents/data-engineer.md" entry. Keep ALL existing and ADD:

**Add section "## Anchor Modeling Physical Patterns"** (expanding the existing "Key Anchor Modeling physical patterns" bullet list into a deeper section):
- Anchor tables: Clustered index on anchor surrogate ID, typically small (just ID column), serve as central join point
- Attribute tables: Composite index on (anchor_id, valid_from), consider range partitioning by valid_from for high-volume historized attributes, separate table per attribute enables independent growth
- Tie tables: Index on both FK columns, consider composite index matching common join patterns, historized ties need (from_anchor_id, valid_from) index
- Knot tables: Small, static -- can be cached or materialized. Index on knot_id
- Nexus tables: Composite index on all participating anchor IDs, consider partitioning for high-volume intersections

---

Update ".opencode/agents/analytics-engineer.md" entry. Keep ALL existing and ADD:

**Add section "## DAB to DAR Mapping Checklist"** (after "## When Invoked"):
- Each anchor -> candidate dimension table
- Each historized attribute -> slowly changing dimension (Type 2)
- Each tie -> dimension hierarchy or fact-dimension relationship
- Each nexus -> fact table candidate (many-to-many intersection with measures)
- Each knot -> conformed dimension or lookup table
- Validate: Every business question identified by @business-analyst can be answered by joining through the bridge table

---

**Add tests** in tests/test_scaffold.py:

1. `test_veteran_reviewer_has_detection_criteria()`: Assert veteran-reviewer.md template contains "Detection criteria" (at least 5 occurrences -- use count method on lowercased string checking for "detection criteria")
2. `test_veteran_reviewer_has_fix_templates()`: Assert veteran-reviewer.md template contains "Fix template" or "fix template" (case-insensitive) with at least 3 occurrences
3. `test_data_engineer_has_physical_patterns()`: Assert data-engineer.md template contains "Anchor Modeling Physical Patterns" or specific index pattern content like "valid_from" and "composite index"
4. `test_analytics_engineer_has_dar_mapping()`: Assert analytics-engineer.md template contains "DAB to DAR" or "Mapping Checklist" and "bridge table"

Important: Do NOT change the template count (14 entries). Only modify existing template string content.
  </action>
  <verify>
Run `cd /workspaces/data-architect && make check` -- all tests pass. Template count still 14. Veteran Reviewer has expanded detection criteria and fix templates. Data Engineer has deepened physical patterns. Analytics Engineer has DAR mapping checklist.
  </verify>
  <done>
Veteran Reviewer's 10-item anti-pattern checklist expanded with detection criteria, example violations, and fix templates for each item. Review protocol specifies first review (after Logical) and final review (after Physical) with focus areas. Data Engineer has deepened Anchor Modeling physical patterns with specific index and partitioning guidance. Analytics Engineer has DAB-to-DAR mapping checklist. Four new tests verify the deepened content.
  </done>
</task>

</tasks>

<verification>
Phase-level checks after this plan:
1. `make check` passes (lint, type, test)
2. TEMPLATES dict still has 14 entries
3. system-analyst.md contains "Source Schema Reading", "Swagger", "OData", "ERD", "burden"
4. business-analyst.md contains "Business Question Evidence", "burden"
5. veteran-reviewer.md contains "Detection criteria" (multiple), "Fix template" (multiple), "Review Protocol"
6. data-engineer.md contains "Anchor Modeling Physical Patterns", "valid_from", "composite index"
7. analytics-engineer.md contains "DAB to DAR", "bridge table"
8. All 6 agent files still start with "---" and have cross-references
</verification>

<success_criteria>
- System Analyst template grew by ~400-600 words with source schema protocols and evidence rules
- Business Analyst template grew by ~300-500 words with evidence templates and burden-of-proof
- Veteran Reviewer template grew by ~800-1200 words with expanded detection criteria and fix templates
- Data Engineer template grew by ~200-300 words with physical patterns
- Analytics Engineer template grew by ~200-300 words with DAR mapping
- 7+ new tests verify the deepened content
- All prior tests still pass
- Template count unchanged at 14
</success_criteria>

<output>
After completion, create `.planning/phases/04-agent-quality-modeling-workflows/04-02-SUMMARY.md`
</output>
