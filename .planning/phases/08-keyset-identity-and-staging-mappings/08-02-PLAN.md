---
phase: 08-keyset-identity-and-staging-mappings
plan: 02
type: execute
wave: 2
depends_on: ["08-01"]
files_modified:
  - src/data_architect/models/staging.py
  - src/data_architect/models/anchor.py
  - src/data_architect/models/common.py
  - src/data_architect/generation/keyset_sql.py
  - src/data_architect/generation/naming.py
  - src/data_architect/generation/ddl.py
  - src/data_architect/generation/dml.py
  - src/data_architect/generation/__init__.py
  - tests/test_staging_models.py
  - tests/test_keyset_sql.py
autonomous: true

must_haves:
  truths:
    - "StagingMapping Pydantic model captures system, tenant, table, natural_key_columns, column_mappings, and optional priority"
    - "Anchor.staging_mappings field accepts list[StagingMapping] instead of list[Any]"
    - "build_keyset_expr() produces CASE WHEN ... IS NULL THEN NULL ELSE CONCAT(...) END SQL with nested REPLACE for delimiter escaping"
    - "Generated DML uses keyset identity column constructed from staging source natural key columns"
    - "DDL staging table generation uses StagingMapping model instead of raw dict"
    - "Column-level staging mappings produce correct INSERT...SELECT with explicit column lineage"
  artifacts:
    - path: "src/data_architect/models/staging.py"
      provides: "StagingMapping and StagingColumn Pydantic models"
      contains: "class StagingMapping"
    - path: "src/data_architect/generation/keyset_sql.py"
      provides: "build_keyset_expr() for SQL keyset construction"
      contains: "build_keyset_expr"
    - path: "tests/test_staging_models.py"
      provides: "Tests for StagingMapping model validation"
    - path: "tests/test_keyset_sql.py"
      provides: "Tests for keyset SQL expression generation across dialects"
  key_links:
    - from: "src/data_architect/generation/keyset_sql.py"
      to: "src/data_architect/identity/escaping.py"
      via: "import escape_delimiters for prefix construction"
      pattern: "from data_architect\\.identity\\.escaping import"
    - from: "src/data_architect/models/anchor.py"
      to: "src/data_architect/models/staging.py"
      via: "staging_mappings: list[StagingMapping]"
      pattern: "from data_architect\\.models\\.staging import"
    - from: "src/data_architect/generation/dml.py"
      to: "src/data_architect/generation/keyset_sql.py"
      via: "import build_keyset_expr for MERGE statements"
      pattern: "from data_architect\\.generation\\.keyset_sql import"
---

<objective>
Staging mapping Pydantic models, keyset SQL generation, and integration with existing DDL/DML pipeline

Purpose: Formalize the staging mapping structure from raw dicts into validated Pydantic models (STG-01, STG-02, STG-03), generate keyset identity construction SQL (KEY-03), and update DDL/DML generators to use typed models with column-level lineage (STG-04).

Output: StagingMapping model, keyset SQL builder, updated DDL/DML generators that produce column-mapped loading SQL with keyset identity construction.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-keyset-identity-and-staging-mappings/08-RESEARCH.md
@.planning/phases/08-keyset-identity-and-staging-mappings/08-01-SUMMARY.md
@src/data_architect/models/anchor.py
@src/data_architect/models/common.py
@src/data_architect/generation/ddl.py
@src/data_architect/generation/dml.py
@src/data_architect/generation/naming.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: StagingMapping Pydantic models and keyset SQL builder</name>
  <files>
    src/data_architect/models/staging.py
    src/data_architect/models/anchor.py
    src/data_architect/generation/keyset_sql.py
    src/data_architect/generation/__init__.py
    tests/test_staging_models.py
    tests/test_keyset_sql.py
  </files>
  <action>
    **1. Create src/data_architect/models/staging.py:**

    Define frozen Pydantic models using FROZEN_CONFIG and yaml_ext_field from common.py:

    ```python
    class StagingColumn(BaseModel):
        """Maps a staging table column to an anchor attribute."""
        model_config = FROZEN_CONFIG
        name: str = yaml_ext_field(description="Source column name in staging table")
        type: str = yaml_ext_field(description="SQL data type")
        maps_to: str | None = yaml_ext_field(default=None, description="Target attribute mnemonic")
    ```

    ```python
    class StagingMapping(BaseModel):
        """Multi-source mapping from a staging table to an anchor."""
        model_config = FROZEN_CONFIG
        system: str = yaml_ext_field(description="Source system identifier")
        tenant: str = yaml_ext_field(description="Tenant identifier")
        table: str = yaml_ext_field(description="Staging table name")
        natural_key_columns: list[str] = yaml_ext_field(description="Columns composing natural key")
        columns: list[StagingColumn] = yaml_ext_field(default_factory=list, description="Column definitions with mappings")
        column_mappings: dict[str, str] = yaml_ext_field(default_factory=dict, description="attribute_mnemonic -> staging_column_name")
        priority: int | None = yaml_ext_field(default=None, description="Conflict resolution priority (lower wins)")
    ```

    Note: `columns` holds the DDL column definitions (name + type), while `column_mappings` holds the DML lineage mapping (attribute_mnemonic -> staging_column). Both are needed: DDL uses `columns` to create the staging table, DML uses `column_mappings` to generate INSERT...SELECT with column lineage.

    **2. Update src/data_architect/models/anchor.py:**

    Change `staging_mappings: list[Any]` to `staging_mappings: list[StagingMapping]`:
    - Add import: `from data_architect.models.staging import StagingMapping`
    - Change field type: `staging_mappings: list[StagingMapping] = yaml_ext_field(default_factory=list, description="Multi-source staging table mappings")`
    - Remove the `Any` import if no longer needed

    **3. Create src/data_architect/generation/keyset_sql.py:**

    ```python
    def build_keyset_expr(entity: str, system: str, tenant: str, natural_key_col: str, dialect: str) -> sge.Expression:
    ```

    Implementation per 08-RESEARCH.md Pattern 3:
    - Import escape_delimiters from data_architect.identity.escaping
    - Escape entity, system, tenant (constants at generation time)
    - Build prefix string: `{esc_entity}@{esc_system}~{esc_tenant}|`
    - Build nested REPLACE for runtime natural key column escaping:
      `REPLACE(REPLACE(REPLACE(nk_col, '@', '@@'), '~', '~~'), '|', '||')`
    - Build CONCAT(prefix_literal, escaped_nk)
    - Wrap in CASE WHEN nk_col IS NULL THEN NULL ELSE concat_expr END
    - Return the CASE expression as sge.Expression

    For composite natural keys (multiple columns), define:
    ```python
    def build_composite_natural_key_expr(columns: list[str], dialect: str) -> sge.Expression:
    ```
    - Concatenate columns with `:` separator (e.g., `col1 || ':' || col2`)
    - Apply CASE WHEN any column IS NULL THEN NULL to propagate NULL (KEY-05)

    **4. Update src/data_architect/generation/__init__.py:**
    - Add export for build_keyset_expr

    **5. Create tests/test_staging_models.py:**
    - Test StagingMapping creation with all fields
    - Test StagingMapping with minimal fields (empty columns, empty column_mappings)
    - Test StagingColumn with maps_to and without
    - Test frozen immutability
    - Test that Anchor.staging_mappings accepts list[StagingMapping]

    **6. Create tests/test_keyset_sql.py:**
    - Test build_keyset_expr produces CASE WHEN ... IS NULL for postgres, tsql, snowflake
    - Test that the SQL contains REPLACE cascade for delimiter escaping
    - Test that prefix contains escaped entity/system/tenant
    - Test build_composite_natural_key_expr for multi-column keys with NULL propagation
    - Test generated SQL parses back correctly via SQLGlot
  </action>
  <verify>
    ```bash
    python -m pytest tests/test_staging_models.py tests/test_keyset_sql.py -v
    make check
    ```
  </verify>
  <done>
    StagingMapping model validates with all required fields. Anchor.staging_mappings is typed as list[StagingMapping]. build_keyset_expr produces dialect-correct NULL-safe keyset construction SQL. All tests pass. make check green.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update DDL/DML generators to use StagingMapping model with column lineage</name>
  <files>
    src/data_architect/generation/naming.py
    src/data_architect/generation/ddl.py
    src/data_architect/generation/dml.py
    tests/test_ddl.py
    tests/test_dml.py
  </files>
  <action>
    **1. Update src/data_architect/generation/naming.py:**

    Change `staging_table_name(mapping: dict[str, Any]) -> str` to accept StagingMapping:
    ```python
    def staging_table_name(mapping: StagingMapping) -> str:
        return mapping.table
    ```
    Import StagingMapping from data_architect.models.staging. Remove the `Any` import if no longer needed.

    **2. Update src/data_architect/generation/ddl.py:**

    In `generate_all_ddl()`, the staging table generation currently reads from raw dicts:
    ```python
    # CURRENT (raw dict):
    columns = [(col["name"], col["type"]) for col in mapping.get("columns", [])]
    ```
    Change to use StagingMapping model:
    ```python
    # NEW (typed model):
    columns = [(col.name, col.type) for col in mapping.columns]
    ```

    The rest of `generate_all_ddl` stays the same since `staging_table_name()` still returns a string.

    **3. Update src/data_architect/generation/dml.py:**

    Update DML builders to use keyset identity when staging_mappings are present:

    In `build_anchor_merge()`:
    - When `anchor.staging_mappings` is populated, use `mapping.table` via `staging_table_name(mapping)` (already works since we updated the function)
    - Add keyset identity column to INSERT: import build_keyset_expr, call it with `mapping.system`, `mapping.tenant`, anchor.descriptor, and the natural key column(s)
    - The keyset replaces metadata_id: `metadata_id` column gets the keyset expression instead of literal `'architect-generated'`

    In `build_attribute_merge()`:
    - When `anchor.staging_mappings` is populated and `column_mappings` exists, use `mapping.column_mappings[attribute.mnemonic]` to look up the staging column name for SELECT
    - This enables explicit column lineage: `SELECT staging_col AS target_col`

    Note: For this plan, update the single-source case (first staging mapping). Multi-source iteration is handled in 08-03.

    **4. Update tests/test_ddl.py:**

    Update any test fixtures that use raw dict staging_mappings to use StagingMapping model instances instead. The test assertions should remain the same since DDL output doesn't change, only the input type.

    **5. Update tests/test_dml.py:**

    Update test fixtures similarly. Add a test verifying that when column_mappings are provided, the generated SQL references staging columns correctly. Add test for keyset identity column in anchor MERGE output.

    **IMPORTANT:** Do NOT break existing tests. The generated SQL format may change slightly to include keyset columns, so update assertions accordingly. The core patterns (MERGE/UPSERT, ON CONFLICT) remain the same.
  </action>
  <verify>
    ```bash
    python -m pytest tests/test_ddl.py tests/test_dml.py tests/test_staging_models.py tests/test_keyset_sql.py -v
    make check
    ```
  </verify>
  <done>
    DDL generator uses StagingMapping model (not raw dicts). DML generator constructs keyset identity in anchor MERGE. Column-level mappings produce explicit INSERT...SELECT with staging column references. All existing tests updated and passing. No raw dict staging_mappings remain in production code. make check green.
  </done>
</task>

</tasks>

<verification>
```bash
# Full test suite
make check

# Verify staging model integration
python -c "from data_architect.models.staging import StagingMapping; print('StagingMapping imported')"
python -c "from data_architect.generation.keyset_sql import build_keyset_expr; print('build_keyset_expr imported')"

# Verify Anchor.staging_mappings type
python -c "
from data_architect.models.anchor import Anchor
import inspect
sig = inspect.signature(Anchor)
print(f'staging_mappings type: {sig.parameters[\"staging_mappings\"].annotation}')
"

# Verify no raw dict staging_mappings in generation code
grep -r "mapping\[" src/data_architect/generation/ || echo "No raw dict access found (good)"
grep -r "mapping.get(" src/data_architect/generation/ || echo "No .get() access found (good)"
```
</verification>

<success_criteria>
- StagingMapping and StagingColumn are frozen Pydantic models importable from data_architect.models.staging
- Anchor.staging_mappings field type is list[StagingMapping] (not list[Any])
- build_keyset_expr produces NULL-safe CASE WHEN SQL for all 3 dialects
- build_composite_natural_key_expr handles multi-column natural keys with NULL propagation
- DDL staging table generation uses StagingMapping.columns (not raw dict)
- DML anchor MERGE includes keyset identity column when staging_mappings present
- Column-level mappings enable explicit INSERT...SELECT lineage
- All existing tests pass (no regressions)
- New tests cover staging models, keyset SQL, and integration
- make check passes
</success_criteria>

<output>
After completion, create `.planning/phases/08-keyset-identity-and-staging-mappings/08-02-SUMMARY.md`
</output>
